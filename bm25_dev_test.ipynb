{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T01:06:11.524380Z",
     "start_time": "2021-10-06T01:06:11.503381Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from setproctitle import setproctitle\n",
    "setproctitle(\"Hodong_PolyEncoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T01:06:15.050204Z",
     "start_time": "2021-10-06T01:06:11.526382Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from transformer.data.retriever_dataset import ElectraDatasetFromDir, RetrieverDataLoader\n",
    "from transformer.tokenizer.utils import make_custom_tokenizer_from_pretrained, load_tokenizer_from_pretrained\n",
    "from transformer.models.interface import TrainHistory\n",
    "from transformer.models.utils import get_score_json\n",
    "from transformer.utils.information_retrieval import BM25Okapi\n",
    "from transformer.utils.common import set_device, convert_to_tensor, convert_to_numpy, init_path, get_last_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set WorkingDirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T01:06:15.065205Z",
     "start_time": "2021-10-06T01:06:15.051174Z"
    }
   },
   "outputs": [],
   "source": [
    "# # AIBUD_DEV\n",
    "# dataset_dir = \"/Users/aibud_dev/_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# # Korea_Server\n",
    "# dataset_dir = \"/home/mnt/guest1\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# bigshane_local\n",
    "dataset_dir = \"D:\\_jupyter\"\n",
    "path = \"./config/file_path.json\"\n",
    "file_path = None\n",
    "with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "    file_path = json.load(fp)\n",
    "\n",
    "# # AWS\n",
    "# dataset_dir = \"/home/ubuntu/data\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T01:06:15.170309Z",
     "start_time": "2021-10-06T01:06:15.068179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained huggingface_tokenizer: 'D:\\_jupyter/huggingface_tokenizer/kor/koelectra-vanila'\n",
      "vocab_size: 32200\n"
     ]
    }
   ],
   "source": [
    "tokenizer_file_path = dataset_dir + \"/huggingface_tokenizer/kor/koelectra-vanila\"\n",
    "\n",
    "# # save tokenizer to local\n",
    "# tokenizer_path = \"monologg/koelectra-base-discriminator\"\n",
    "# add_special_token = True\n",
    "# tokenizer = make_custom_tokenizer_from_pretrained(model_type=\"electra\", name_or_path=tokenizer_path, add_special_token=add_special_token)\n",
    "# tokenizer.save_pretrained(tokenizer_file_path)\n",
    "\n",
    "tokenizer = load_tokenizer_from_pretrained(model_type=\"electra\", name_or_path=tokenizer_file_path)\n",
    "print(\"vocab_size:\", len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T01:06:28.319067Z",
     "start_time": "2021-10-06T01:06:15.172465Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing data: 100%|█████████████████████████████████████████████████████████| 6872/6872 [00:12<00:00, 533.63it/s]\n"
     ]
    }
   ],
   "source": [
    "timesteps = 128\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "nprocs = 1\n",
    "\n",
    "dataset_name = \"four_n2x8_both\"\n",
    "total_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/{}/\".format(dataset_name)\n",
    "sample_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/{}/sample/\".format(dataset_name)\n",
    "train_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/{}/train/\".format(dataset_name)\n",
    "val_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/{}/val/\".format(dataset_name)\n",
    "test_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/{}/test/\".format(dataset_name)\n",
    "\n",
    "# total_dataset = ElectraDatasetFromDir(data_dir=total_data_dir, tokenizer=tokenizer, timesteps=timesteps, batch_size=batch_size, device=device, nprocs=nprocs)\n",
    "\n",
    "# train_dataset = ElectraDatasetFromDir(data_dir=train_data_dir, tokenizer=tokenizer, timesteps=timesteps, batch_size=batch_size, device=device, nprocs=nprocs)\n",
    "# train_data_loader = RetrieverDataLoader(dataset=train_dataset, batch_size=batch_size, device=device)\n",
    "\n",
    "# val_dataset = ElectraDatasetFromDir(data_dir=val_data_dir, tokenizer=tokenizer, timesteps=timesteps, batch_size=batch_size, device=device, nprocs=nprocs)\n",
    "# val_data_loader = RetrieverDataLoader(dataset=val_dataset, batch_size=batch_size, device=device)\n",
    "\n",
    "test_dataset = ElectraDatasetFromDir(data_dir=test_data_dir, tokenizer=tokenizer, timesteps=timesteps, batch_size=batch_size, device=device, nprocs=nprocs)\n",
    "test_data_loader = RetrieverDataLoader(dataset=test_dataset, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T01:06:32.684656Z",
     "start_time": "2021-10-06T01:06:28.320068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'temp_dir' has been set to './20211006_100628/' to save model while training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bigshane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "candidates = []\n",
    "for row in test_dataset.raw_data:\n",
    "    last_index = get_last_index(row[\"speaker_ids\"], value=test_dataset.user_speaker_id)\n",
    "    candidate = row[\"utterances\"][last_index+1:]\n",
    "    candidate = \" \".join(candidate)\n",
    "    candidates.append(candidate)\n",
    "\n",
    "bm25 = BM25Okapi(tokenizer=tokenizer, candidates=candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T01:18:47.701383Z",
     "start_time": "2021-10-06T01:06:32.685274Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load beomi/kcbert-base with 4 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing scores: 100%|████████████████████████████████████████████████████████████| 6872/6872 [11:55<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval (cuda:0) [-1 /-1 ]: (loss)  | (acc) HITS@1: 6.370e-02, HITS@2: 8.440e-02, HITS@5: 1.267e-01, HITS@10: 1.781e-01, BERTScore: 6.782e-01,  | train_time: 0.0s, last_lr: -1.0000000000\n"
     ]
    }
   ],
   "source": [
    "metrics = [\"hits\", \"semantic_score\"]\n",
    "hits_k = [1,2,5,10]\n",
    "model_name = \"BM25Okapi\"\n",
    "\n",
    "model_dir = dataset_dir + \"/model/bm25/{dataset_name}/\".format(dataset_name=dataset_name)\n",
    "log_dir = dataset_dir + \"/essay/bm25/{dataset_name}/\".format(dataset_name=dataset_name)\n",
    "init_path(log_dir, reset=True)\n",
    "\n",
    "scores = bm25.compute_scores(metrics=metrics, tokenizer=tokenizer, data_loader=test_data_loader, device=device, hits_k=hits_k)\n",
    "output_json = get_score_json(model_name=model_name, dataset_name=dataset_name, test_data_size=len(test_dataset.raw_data), batch_size=batch_size, scores=scores)\n",
    "\n",
    "# verbose & append log\n",
    "eval_history = TrainHistory()\n",
    "loss_dict = dict()\n",
    "acc_dict = dict()\n",
    "for metric, metric_score in scores.items():\n",
    "    acc_dict[metric] = metric_score\n",
    "eval_history.update(loss_dict=loss_dict, acc_dict=acc_dict, lr=-1)\n",
    "eval_str = bm25.verbose_template.format(mode=\"Eval\", device=device, idx=-1, num_iters=-1) + str(eval_history)\n",
    "print(eval_str)\n",
    "\n",
    "with open(log_dir + \"/score_logs.txt\", \"a\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(eval_str + \"\\n\")\n",
    "\n",
    "# write detailed logs\n",
    "init_path(log_dir, reset=False)\n",
    "init_path(log_dir + \"/detailed/\", reset=True)\n",
    "with open(log_dir + \"/detailed/score_logs.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dumps(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = [\n",
    "    \"안녕하세요\",\n",
    "#     \"무슨 일로 저에게 상담을 신청하셨나요?\"\n",
    "#     \"요즘 인간관계가 고민이에요.\",\n",
    "#     \"어떤 고민이죠?\",\n",
    "#     \"친구들이랑 연락도 뜸해지고 자주 못만나서 서먹해지는 것 같아요\",\n",
    "#     \"이래저래 연락하기 힘드신가봐요\",\n",
    "#     \"네, 코로나 때문에 만나질 못해서 더 혼자가 된 느낌이에요.\",\n",
    "#     \"저도 지쳐요.\",\n",
    "#     \"당신도 사람들을 자주 못 만나시나봐요\"\n",
    " ]\n",
    "speaker_ids = [(i+1)%2 for i in range(len(utterances))]\n",
    "\n",
    "outputs = service.infer_next_utterance(utterances, speaker_ids, 10, 5, None, 0.5, 5)\n",
    "outputs[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
