{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T14:01:56.179966Z",
     "start_time": "2021-10-01T14:01:53.989802Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from bs4 import BeautifulSoup, element\n",
    "\n",
    "from transformer.utils.tokenizer import MecabTokenizer, SpmTokenizer\n",
    "from transformer.data.dataset import DatasetInterface, DatasetFromDir\n",
    "# from transformer.preprocessors.bert_preprocessor import BertPreprocessor\n",
    "# from transformer.preprocessors.blender_bot_preprocessor import GeneratorPretrainingPreprocessor\n",
    "from transformer.preprocessors.utils import split_segment_by_speaker_ids, convert_turn_ids, flatten_sequence\n",
    "from transformer.utils.common import get_nth_index, get_last_index, init_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T14:01:56.194965Z",
     "start_time": "2021-10-01T14:01:56.180967Z"
    }
   },
   "outputs": [],
   "source": [
    "# # AIBUD_DEV\n",
    "# dataset_dir = \"/Users/aibud_dev/_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# # AWS\n",
    "# dataset_dir = \"/home/ubuntu/data\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# # Korea_Server\n",
    "# dataset_dir = \"/home/mnt/guest1\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# bigshane_local\n",
    "dataset_dir = \"D:\\_jupyter\"\n",
    "path = \"./config/file_path.json\"\n",
    "file_path = None\n",
    "with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "    file_path = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T14:01:56.225527Z",
     "start_time": "2021-10-01T14:01:56.195965Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣{emojis}]+')\n",
    "url_pattern = re.compile(\n",
    "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "def clean(x):\n",
    "    x = pattern.sub(' ', x)\n",
    "    x = url_pattern.sub('', x)\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x\n",
    "\n",
    "def trim_obj(obj):\n",
    "    obj = list(set(obj))\n",
    "    obj = [e for e in obj if e!=\"\"]\n",
    "    obj = sorted(obj)\n",
    "    return obj\n",
    "    \n",
    "def show_five_nums(data):\n",
    "    quartiles = np.percentile(data, [25, 50, 75])\n",
    "    min_v = np.min(data)\n",
    "    max_v = np.max(data)\n",
    "    avg = np.mean(data)\n",
    "    print(\"Min: {min_v:.3f}\\tMax: {max_v:.3f}\\tAvg: {avg:.3f}\\tQ1: {q1:.3f}\\tQ2: {q2:.3f}\\tQ3: {q3:.3f}\".format(min_v=min_v, max_v=max_v, avg=avg, q1=quartiles[0], q2=quartiles[1], q3=quartiles[2]))\n",
    "    return min_v, max_v, quartiles[0], quartiles[1], quartiles[2], avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wriet as Json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T14:01:56.240525Z",
     "start_time": "2021-10-01T14:01:56.226529Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open(root_dir + \"/{language}/multi_turn/feed_data_0.json\".format(language=language), \"w\", encoding=encoding) as fp:\n",
    "#     json.dump(output, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dialog finetuning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T14:16:18.435861Z",
     "start_time": "2021-10-01T14:16:18.422834Z"
    }
   },
   "outputs": [],
   "source": [
    "min_num_turns = 2\n",
    "max_num_turns = 8\n",
    "shuffle = True\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.05\n",
    "assert train_ratio+val_ratio+test_ratio == 1, \"Sum must be equal to 1\"\n",
    "\n",
    "\n",
    "num_samples = 5000\n",
    "append_condition = True\n",
    "\n",
    "dataset_name_list = [\"SelectStar\", \"EmpatheticDialogues\", \"AIHUB_Twitter\", \"AIHUB_EMOTION\", \"AIHUB_SSGI\", \"AIHUB_SNS\", \"KLUE_DST\"]; _dataset_name = \"eight\"\n",
    "# dataset_name_list = [\"SelectStar\", \"EmpatheticDialogues\", \"AIHUB_Twitter\", \"AIHUB_EMOTION\"]; _dataset_name = \"four\"\n",
    "# dataset_name_list = [\"SelectStar\", \"EmpatheticDialogues\"]; _dataset_name = \"selectstar\"\n",
    "# dataset_name_list = [\"SelectStarPersona\"]; _dataset_name = \"persona\"\n",
    "_output_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/kor/{_dataset_name}_n{min_v}x{max_v}\".format(_dataset_name=_dataset_name, min_v=min_num_turns, max_v=max_num_turns)\n",
    "_retriever_output_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/retriever/{_dataset_name}_n{min_v}x{max_v}\".format(_dataset_name=_dataset_name, min_v=min_num_turns, max_v=max_num_turns)\n",
    "_generator_output_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/generator/{_dataset_name}_n{min_v}x{max_v}\".format(_dataset_name=_dataset_name, min_v=min_num_turns, max_v=max_num_turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T14:16:24.240840Z",
     "start_time": "2021-10-01T14:16:19.718457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Load data from datasets...\n",
      "\tSelectStar: 5692\t\tavg(turn_size): 10.733661278988054\t\tavg(utt_size): 33.66426563598032\n",
      "\tEmpatheticDialogues: 618\t\tavg(turn_size): 5.699029126213592\t\tavg(utt_size): 18.25242718446602\n",
      "\tAIHUB_Twitter: 1999\t\tavg(turn_size): 3.1160580290145075\t\tavg(utt_size): 6.789894947473737\n",
      "\tAIHUB_EMOTION: 84212\t\tavg(turn_size): 2.6633021422125114\t\tavg(utt_size): 5.326818030684463\n",
      "\tAIHUB_SSGI: 5855\t\tavg(turn_size): 6.824935952177626\t\tavg(utt_size): 15.44201537147737\n",
      "\tAIHUB_SNS: 36680\t\tavg(turn_size): 2.788794983642312\t\tavg(utt_size): 11.172001090512541\n",
      "\tKLUE_DST: 9000\t\tavg(turn_size): 7.335111111111111\t\tavg(utt_size): 14.670222222222222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "extension = \"json\"\n",
    "print(\"# Load data from datasets...\")\n",
    "stat_dict = OrderedDict()\n",
    "data = OrderedDict()\n",
    "\n",
    "for dataset_name in dataset_name_list:\n",
    "    root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "    if \"multi_turn\" not in file_path[dataset_name][\"feed_data\"]: continue\n",
    "\n",
    "    data_path = file_path[dataset_name][\"feed_data\"][\"multi_turn\"].format(root_dir=root_dir, language=language)\n",
    "    for file_name in os.listdir(data_path):\n",
    "        if not file_name.endswith(extension): continue\n",
    "        with open(data_path+file_name, \"r\", encoding=encoding) as fp:\n",
    "            _data = json.load(fp)\n",
    "            _data = [r for r in _data if len(r[\"speaker_ids\"]) > 0]\n",
    "\n",
    "            if dataset_name not in data: data[dataset_name] = []\n",
    "            data[dataset_name] += _data\n",
    "\n",
    "            _turn_size_list = []\n",
    "            _utt_size_list = []\n",
    "            for r in _data:\n",
    "                _turn_size = 0\n",
    "                last_speaker_id = prev_speaker_id = r[\"speaker_ids\"][-1]\n",
    "                for speaker_id in r[\"speaker_ids\"][::-1]:\n",
    "                    if speaker_id != last_speaker_id and speaker_id != prev_speaker_id: _turn_size += 1\n",
    "                    prev_speaker_id = speaker_id\n",
    "                _turn_size_list.append(_turn_size)\n",
    "                _utt_size_list.append(len(r[\"speaker_ids\"]))\n",
    "            _avg_turn_size = np.mean(_turn_size_list)\n",
    "            _avg_utt_size = np.mean(_utt_size_list)\n",
    "            \n",
    "            print(\"\\t{}: {}\\t\\tavg(turn_size): {}\\t\\tavg(utt_size): {}\".format(dataset_name, len(_data), _avg_turn_size, _avg_utt_size))\n",
    "            stat_dict[dataset_name] = dict()\n",
    "            stat_dict[dataset_name][\"dataset_size\"] = len(_data)\n",
    "            stat_dict[dataset_name][\"avg_turns\"] = _avg_turn_size\n",
    "            stat_dict[dataset_name][\"avg_utterances\"] = _avg_utt_size\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T14:16:24.571415Z",
     "start_time": "2021-10-01T14:16:24.241812Z"
    }
   },
   "outputs": [],
   "source": [
    "retriever_data = dict()\n",
    "generator_data = dict()\n",
    "for dataset_name, _data in data.items():\n",
    "    random.shuffle(_data)\n",
    "    split_idx = len(_data)//2\n",
    "    retriever_data[dataset_name] = _data[:split_idx]\n",
    "    generator_data[dataset_name] = _data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T14:22:57.714301Z",
     "start_time": "2021-10-01T14:22:57.701296Z"
    }
   },
   "outputs": [],
   "source": [
    "data = retriever_data\n",
    "_output_dir = _retriever_output_dir\n",
    "\n",
    "# data = generator_data\n",
    "# _output_dir = _generator_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T14:25:53.388554Z",
     "start_time": "2021-10-01T14:25:12.846481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Extract utterances from data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SelectStar: 100%|█████████████████████████████████████████████████████████████████| 2846/2846 [00:05<00:00, 536.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSelectStar : (train: 29951, val: 5637, test: 1904) (empty_row_cnt: 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EmpatheticDialogues: 100%|█████████████████████████████████████████████████████████| 309/309 [00:00<00:00, 3153.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEmpatheticDialogues : (train: 876, val: 164, test: 56) (empty_row_cnt: 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AIHUB_Twitter: 100%|██████████████████████████████████████████████████████████████| 999/999 [00:00<00:00, 16933.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAIHUB_Twitter : (train: 1257, val: 348, test: 66) (empty_row_cnt: 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AIHUB_EMOTION: 100%|██████████████████████████████████████████████████████████| 42106/42106 [00:00<00:00, 50844.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAIHUB_EMOTION : (train: 22386, val: 4119, test: 1429) (empty_row_cnt: 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AIHUB_SSGI: 100%|██████████████████████████████████████████████████████████████████| 2927/2927 [00:31<00:00, 92.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAIHUB_SSGI : (train: 13519, val: 2664, test: 937) (empty_row_cnt: 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AIHUB_SNS: 100%|██████████████████████████████████████████████████████████████| 18340/18340 [00:01<00:00, 12698.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAIHUB_SNS : (train: 10041, val: 1870, test: 609) (empty_row_cnt: 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KLUE_DST: 100%|██████████████████████████████████████████████████████████████████| 4500/4500 [00:00<00:00, 4896.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKLUE_DST : (train: 21151, val: 4209, test: 1328) (empty_row_cnt: 0)\n"
     ]
    }
   ],
   "source": [
    "print(\"# Extract utterances from data...\")\n",
    "train_output = []\n",
    "val_output = []\n",
    "test_output = []\n",
    "\n",
    "model_speaker_id = 1\n",
    "user_speaker_id = 0\n",
    "\n",
    "for dataset_name, _data in data.items():\n",
    "    _output = []\n",
    "    _train_output = []\n",
    "    _val_output = []\n",
    "    _test_output = []\n",
    "    \n",
    "    train_idx = int(len(_data) * train_ratio)\n",
    "    val_idx = int(len(_data) * (train_ratio+val_ratio))\n",
    "    \n",
    "    empty_row_cnt = 0\n",
    "    for row_idx, row in enumerate(tqdm(_data, initial=0, total=len(_data), desc=dataset_name)):\n",
    "        if len([_utterance for _utterance in row[\"utterances\"] if _utterance!=\"\"]) < 1:\n",
    "            empty_row_cnt += 1\n",
    "            continue\n",
    "\n",
    "        if \"labels\" not in row:\n",
    "            labels = [\"\"] * len(row[\"utterances\"])\n",
    "            row[\"labels\"] = labels\n",
    "\n",
    "        for begin_idx in range(0, max((len(row[\"speaker_ids\"]) - min_num_turns), (len(row[\"speaker_ids\"]) - max_num_turns))):\n",
    "            for end_idx in range(begin_idx+1, len(row[\"speaker_ids\"])):\n",
    "                cur_num_turns = 0\n",
    "                _prev_speaker_id = row[\"speaker_ids\"][begin_idx]\n",
    "                for _speaker_id in row[\"speaker_ids\"][begin_idx+1:end_idx+1]:\n",
    "                    if _speaker_id != _prev_speaker_id:\n",
    "                        _prev_speaker_id = _speaker_id\n",
    "                        if _speaker_id == user_speaker_id: cur_num_turns += 1\n",
    "\n",
    "                if begin_idx <= 0:\n",
    "                    if cur_num_turns < min_num_turns: continue # 대화의 첫 발화이면 min_num_turns턴 미만인 경우에 continue\n",
    "                    if cur_num_turns > max_num_turns: continue # 대화의 첫 발화이면 max_num_turns턴 초과인 경우에 continue\n",
    "                else:\n",
    "                    # if cur_num_turns < min_num_turns: continue # 대화의 첫 발화가 아니면 min_num_turns턴 미만인 경우에 continue\n",
    "                    # if cur_num_turns > max_num_turns: continue # 대화의 첫 발화가 아니면 max_num_turns턴 초과인 경우에 continue\n",
    "                    if cur_num_turns < max_num_turns: continue # 대화의 첫 발화가 아니면 max_num_turns에 맞게 윈도윙\n",
    "                if cur_num_turns > max_num_turns: break # max_num_turns보다 크면 break\n",
    "                if row[\"speaker_ids\"][end_idx] != model_speaker_id: continue # model_id로 발화가 끝나지 않는 경우 continue\n",
    "                if len(row[\"speaker_ids\"]) > end_idx+1 and row[\"speaker_ids\"][end_idx] == row[\"speaker_ids\"][end_idx+1]: continue # 다음 발화도 model_id의 것이면 continue\n",
    "\n",
    "                _utterances = row[\"utterances\"][begin_idx:end_idx+1]\n",
    "                _speaker_ids = row[\"speaker_ids\"][begin_idx:end_idx+1]\n",
    "                _labels = row[\"labels\"][begin_idx:end_idx+1]\n",
    "                _persona = row[\"persona\"] if \"persona\" in row else None\n",
    "                if isinstance(_persona, dict): _persona = list(_persona.values())\n",
    "                _entities = dict()\n",
    "                if \"entities\" in row:\n",
    "                    for k,v in row[\"entities\"].items():\n",
    "                        _entities[k] = []\n",
    "                        for _v in v:\n",
    "                            _entitiy_row_idx, _entitiy_begin_idx, _entitiy_end_idx, _entitiy_span = _v\n",
    "                            entity_row_idx = _entitiy_row_idx - begin_idx\n",
    "                            if entity_row_idx < 0: continue\n",
    "                            if entity_row_idx > (end_idx - begin_idx): continue\n",
    "                            _v_row = [entity_row_idx, _entitiy_begin_idx, _entitiy_end_idx, _entitiy_span]\n",
    "                            _entities[k].append(_v_row)\n",
    "\n",
    "                # utterance_length constraints: timestep보다 길면 탈락 (128 - num_special_tokens)\n",
    "#                 concat_utterances = \" \".join([str(_utterance) for _utterance in _utterances])\n",
    "#                 if preprocessor.get_src_token_length(sentence=concat_utterances) >= 125: continue\n",
    "\n",
    "                row_dict = row.copy()\n",
    "                row_dict[\"utterances\"] = _utterances\n",
    "                row_dict[\"speaker_ids\"] = _speaker_ids\n",
    "                row_dict[\"labels\"] = _labels\n",
    "                row_dict[\"entities\"] = _entities\n",
    "                row_dict[\"persona\"] = _persona\n",
    "#                 _conditions = get_condition(utterances=_utterances, speaker_ids=_speaker_ids, personas=_persona, entities=_entities, user_speaker_id=user_speaker_id)\n",
    "                _conditions = None\n",
    "                row_dict[\"conditions\"] = _conditions\n",
    "\n",
    "#                 u_reply = True if \"name\" in row_dict[\"entities\"] and len(row_dict[\"entities\"][\"name\"]) > 0 and row_dict[\"entities\"][\"name\"][-1][0] == len(row_dict[\"utterances\"])-1 else False\n",
    "                if row_idx < train_idx:\n",
    "                    _train_output.append(row_dict)\n",
    "                elif row_idx < val_idx:\n",
    "                    _val_output.append(row_dict)\n",
    "                else:\n",
    "                    _test_output.append(row_dict)\n",
    "    train_output += _train_output\n",
    "    val_output += _val_output\n",
    "    test_output += _test_output\n",
    "    print(\"\\t{} : (train: {}, val: {}, test: {})\".format(dataset_name, len(_train_output), len(_val_output), len(_test_output)), \"(empty_row_cnt: {})\".format(empty_row_cnt))\n",
    "    stat_dict[dataset_name][\"dialog_size\"] = (len(_train_output), len(_val_output), len(_test_output))\n",
    "\n",
    "random.shuffle(train_output)\n",
    "random.shuffle(val_output)\n",
    "random.shuffle(test_output)\n",
    "output = train_output + val_output + test_output\n",
    "random.shuffle(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T14:23:38.614104Z",
     "start_time": "2021-10-01T14:23:37.970100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\_jupyter/dataset/preprocessed/dialog_finetuning/retriever/eight_n2x8_one/: 132682, 105754, 20174, 6754\n"
     ]
    }
   ],
   "source": [
    "# model_speaker_id = 0 # user_speaker_id = 1\n",
    "final_output = [] \n",
    "final_train_output = [] \n",
    "final_val_output = [] \n",
    "final_test_output = [] \n",
    "final_output += output\n",
    "final_train_output += train_output\n",
    "final_val_output += val_output\n",
    "final_test_output += test_output\n",
    "output_dir = _output_dir + \"_one/\"\n",
    "print(\"{}: {}, {}, {}, {}\".format(output_dir, len(output), len(train_output), len(val_output), len(test_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T14:25:53.418559Z",
     "start_time": "2021-10-01T14:25:53.391454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\_jupyter/dataset/preprocessed/dialog_finetuning/retriever/eight_n2x8_both/: 257203, 204935, 39185, 6754\n"
     ]
    }
   ],
   "source": [
    "# model_speaker_id = 1 # user_speaker_id = 0\n",
    "final_output += output\n",
    "final_train_output += train_output\n",
    "final_val_output += val_output\n",
    "output = final_output\n",
    "train_output = final_train_output\n",
    "val_output = final_val_output\n",
    "test_output = final_test_output\n",
    "output_dir = _output_dir + \"_both/\"\n",
    "print(\"{}: {}, {}, {}, {}\".format(output_dir, len(output), len(train_output), len(val_output), len(test_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T14:26:24.491393Z",
     "start_time": "2021-10-01T14:25:53.420562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Write extracted dialogs to 'D:\\_jupyter/dataset/preprocessed/dialog_finetuning/retriever/eight_n2x8_both/'...\n",
      "\tfeed_data_00.json: 100000\n",
      "\tfeed_data_01.json: 100000\n",
      "\tfeed_data_02.json: 57203\n",
      "\ttrain feed_data_00.json: 100000\n",
      "\ttrain feed_data_01.json: 100000\n",
      "\ttrain feed_data_02.json: 4935\n",
      "\tval feed_data_00.json: 39185\n",
      "\ttest feed_data_00.json: 6754\n"
     ]
    }
   ],
   "source": [
    "output_filename_template = \"feed_data_{idx}.json\"\n",
    "size_per_file = 100000\n",
    "\n",
    "print(\"# Write extracted dialogs to '{}'...\".format(output_dir))\n",
    "init_path(output_dir, reset=True)\n",
    "if shuffle: random.shuffle(output)\n",
    "for idx in range(0, len(output)//size_per_file+1):\n",
    "    begin_idx = idx * size_per_file\n",
    "    end_idx = min(len(output), (idx+1) * size_per_file)\n",
    "    _output = output[begin_idx:end_idx]\n",
    "    output_filename = output_filename_template.format(idx=str(idx).zfill(2))\n",
    "    with open(output_dir+output_filename, \"w\", encoding=\"utf-8\") as fp:\n",
    "        print(\"\\t{}: {}\".format(output_filename, len(_output)))\n",
    "        json.dump(_output, fp)\n",
    "\n",
    "if not os.path.isdir(output_dir + \"train\"): os.mkdir(output_dir + \"train\")\n",
    "if shuffle: random.shuffle(train_output)\n",
    "for idx in range(0, len(train_output)//size_per_file+1):\n",
    "    begin_idx = idx * size_per_file\n",
    "    end_idx = min(len(train_output), (idx+1) * size_per_file)\n",
    "    _output = train_output[begin_idx:end_idx]\n",
    "    output_filename = output_filename_template.format(idx=str(idx).zfill(2))\n",
    "    with open(output_dir+\"train/\"+output_filename, \"w\", encoding=\"utf-8\") as fp:\n",
    "        print(\"\\ttrain {}: {}\".format(output_filename, len(_output)))\n",
    "        json.dump(_output, fp)\n",
    "        \n",
    "if not os.path.isdir(output_dir + \"val\"): os.mkdir(output_dir + \"val\")\n",
    "if shuffle: random.shuffle(val_output)\n",
    "for idx in range(0, len(val_output)//size_per_file+1):\n",
    "    begin_idx = idx * size_per_file\n",
    "    end_idx = min(len(val_output), (idx+1) * size_per_file)\n",
    "    _output = val_output[begin_idx:end_idx]\n",
    "    output_filename = output_filename_template.format(idx=str(idx).zfill(2))\n",
    "    with open(output_dir+\"val/\"+output_filename, \"w\", encoding=\"utf-8\") as fp:\n",
    "        print(\"\\tval {}: {}\".format(output_filename, len(_output)))\n",
    "        json.dump(_output, fp)\n",
    "        \n",
    "if not os.path.isdir(output_dir + \"test\"): os.mkdir(output_dir + \"test\")\n",
    "if shuffle: random.shuffle(test_output)\n",
    "for idx in range(0, len(test_output)//size_per_file+1):\n",
    "    begin_idx = idx * size_per_file\n",
    "    end_idx = min(len(test_output), (idx+1) * size_per_file)\n",
    "    _output = test_output[begin_idx:end_idx]\n",
    "    output_filename = output_filename_template.format(idx=str(idx).zfill(2))\n",
    "    with open(output_dir+\"test/\"+output_filename, \"w\", encoding=\"utf-8\") as fp:\n",
    "        print(\"\\ttest {}: {}\".format(output_filename, len(_output)))\n",
    "        json.dump(_output, fp)\n",
    "        \n",
    "# write sample data\n",
    "if not os.path.isdir(output_dir + \"sample\"): os.mkdir(output_dir + \"sample\")\n",
    "sample_data_path = output_dir + \"sample/feed_data_sample.json\"\n",
    "with open(sample_data_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(output[:num_samples], fp)\n",
    "    \n",
    "# write DESC.md\n",
    "with open(output_dir + \"DESC.md\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    _str = \"## DESC\\n\" +\\\n",
    "        \"- Korean Dialog dataset for Language Model training\\n\" +\\\n",
    "        \"- total: {size}, avg_turns: {avg:.3f}\\n\".format(size=len(output), avg=np.mean([len(row[\"utterances\"]) for row in output])) +\\\n",
    "        \"- min turns: {mn_t}, max turns: {mx_t}\\n\".format(mn_t=min_num_turns, mx_t=max_num_turns) +\\\n",
    "        \"\\n\" +\\\n",
    "        \"## dataset statistics\\n\" +\\\n",
    "        \"### dataset_name: (avg_turns, avg_utterances, num_dialogues, num_total_rows, num_train_rows, num_val_rows, num_test_rows)\\n\"\n",
    "    for dataset_name, stat in stat_dict.items():\n",
    "        _row_stat_template = \"\\t- {dataset_name}: ({avg_turns:.3f}, {avg_utterances:.3f}, {num_dialogues}, {num_total_rows}, {num_train_rows}, {num_val_rows}, {num_test_rows})\\n\"\n",
    "        _row_stat = _row_stat_template.format(dataset_name=dataset_name, avg_turns=stat[\"avg_turns\"], avg_utterances=stat[\"avg_utterances\"], num_dialogues=stat[\"dataset_size\"], num_total_rows=sum(stat[\"dialog_size\"]), num_train_rows=stat[\"dialog_size\"][0], num_val_rows=stat[\"dialog_size\"][1], num_test_rows=stat[\"dialog_size\"][2])\n",
    "        _str += _row_stat\n",
    "    fp.write(_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_utterance = {\n",
    "    \"P\": \"즐거움\",\n",
    "    \"J\": \"기쁨\",\n",
    "    \"S\": \"슬픔\",\n",
    "    \"A\": \"분노\",\n",
    "    \"O\": \"보통\"\n",
    "}\n",
    "g_utterance = {\n",
    "    \"U\": \"유저정보\",\n",
    "    \"R\": \"페르소나\",\n",
    "    \"K\": \"외부지식\",\n",
    "    \"E\": \"공감형대화\",\n",
    "    \"N\": \"일반/기타\"\n",
    "}\n",
    "\n",
    "from collections import Counter\n",
    "labels = [label for row in output for label in row[\"labels\"]]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.services.dialog_retriever.poly_encoder import PolyEncoderDialogRetriever\n",
    "dataset_name = \"four_n2x8_both\"\n",
    "_epoch = 10\n",
    "_model_dir = dataset_dir + \"/model/poly_encoder/v3/concat/{dataset_name}/epoch_{_epoch}/\".format(dataset_name=dataset_name, _epoch=_epoch)\n",
    "\n",
    "service = PolyEncoderDialogRetriever()\n",
    "service.verbose = False\n",
    "service.set_device(device=\"cuda:0\")\n",
    "service.load_model(model_dir=_model_dir)\n",
    "\n",
    "def get_condition_from_retriever(utterances, speaker_ids, min_length=10, top_n=5, weight_bm25=True, prev_utterance=None, intersection_tolerance=0.9, max_retry=5):\n",
    "    conditions = None\n",
    "    try:\n",
    "#         outputs = service.infer_next_utterance_bm25(utterances, min_length=min_length, top_n=top_n, prev_utterance=prev_utterance, intersection_tolerance=intersection_tolerance)\n",
    "        outputs = service.infer_next_utterance(utterances, speaker_ids, min_length=min_length, top_n=top_n, weight_bm25=weight_bm25, prev_utterance=prev_utterance, intersection_tolerance=intersection_tolerance, max_retry=max_retry)\n",
    "        conditions = [outputs[0][0]]\n",
    "    except Exception as ex:\n",
    "        print(\"{}: {}\".format(type(ex), ex))\n",
    "#         conditions = utterances[last_user_idx+1:]\n",
    "    return conditions\n",
    "\n",
    "def get_condition(utterances, speaker_ids, persona, entities, user_speaker_id):\n",
    "    is_user_condition = False\n",
    "    name_in_utterance = None\n",
    "    conditions = None\n",
    "    last_index = get_last_index(speaker_ids, value=user_speaker_id)\n",
    "\n",
    "    if persona is not None and entities is not None:\n",
    "        user_persona = [_persona for _persona in persona if _persona[\"id\"]==user_speaker_id][0]\n",
    "        if \"name\" in entities and len(entities[\"name\"]) > 0:\n",
    "            last_entity_info = entities[\"name\"][-1]\n",
    "            entity_row_idx, entity_begin_idx, entity_end_idx, entity_span = last_entity_info\n",
    "            name_in_utterance = utterances[entity_row_idx][entity_begin_idx:entity_end_idx]\n",
    "            if entity_row_idx in range(last_index+1, len(utterances)) and name_in_utterance != \"\" and name_in_utterance in user_persona[\"name\"]:\n",
    "                is_user_condition = True\n",
    "\n",
    "    if is_user_condition:\n",
    "        # 유저 정보를 이용하면, 컨디션으로 유저 정보를 넣어줘야지\n",
    "        # user_name = user_persona[\"name\"]\n",
    "        # name_condition = {\"name\": name_in_utterance}\n",
    "        # name_condition = name_condition.__str__()\n",
    "        name_condition = \"상대의 이름은 {name}입니다.\".format(name=name_in_utterance)\n",
    "        conditions = [name_condition]\n",
    "    else:\n",
    "#         conditions = None\n",
    "        conditions = get_condition_from_retriever(utterances=utterances[:last_index+1], speaker_ids=speaker_ids[:last_index+1])\n",
    "    return conditions\n",
    "\n",
    "def insert_condition(row, user_speaker_id, use_condition):\n",
    "    row_dict = row.copy()\n",
    "    utterances = row[\"utterances\"]\n",
    "    speaker_ids = row[\"speaker_ids\"]\n",
    "    last_index = get_last_index(speaker_ids, value=user_speaker_id)\n",
    "\n",
    "    persona = None\n",
    "    entities = None\n",
    "    if use_condition and \"persona\" in row: persona = row[\"perosna\"]\n",
    "    if use_condition and \"entities\" in row: persona = row[\"entities\"]\n",
    "    conditions = get_condition(utterances=utterances[:last_index+1], speaker_ids=speaker_ids[:last_index+1], persona=persona, entities=entities, user_speaker_id=user_speaker_id)\n",
    "    row_dict[\"conditions\"] = conditions\n",
    "    return row_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/generator/{dataset_name}/\".format(dataset_name=dataset_name)\n",
    "output_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/generator/condition/{dataset_name}/\".format(dataset_name=dataset_name)\n",
    "# dir_postfix_list = [\"train/\", \"val/\", \"test/\", \"sample/\", \"\"]\n",
    "dir_postfix_list = [\"train/\", \"sample/\"]\n",
    "# dir_postfix_list = [\"val/\", \"test/\", \"\"]\n",
    "\n",
    "model_speaker_id = 0\n",
    "user_speaker_id = 1\n",
    "use_condition = False\n",
    "\n",
    "for dir_postfix in dir_postfix_list:\n",
    "    print(\"data_dir: {}\".format(input_dir+dir_postfix))\n",
    "    for filename in os.listdir(input_dir+dir_postfix):\n",
    "        print(\"\\tfilename: {}\".format(filename))\n",
    "        if os.path.isdir(input_dir+dir_postfix + filename): continue\n",
    "        if not filename.endswith(\".json\"): continue\n",
    "\n",
    "        _data = None\n",
    "        with open(input_dir + dir_postfix + filename, \"r\", encoding=\"utf-8\") as fp:\n",
    "            _data = json.load(fp)\n",
    "\n",
    "        if _data is None or len(_data) <= 0: continue\n",
    "        data = []\n",
    "        for row_idx, row in tqdm(enumerate(_data), initial=0, total=len(_data)):\n",
    "            row.pop(\"conditions\")\n",
    "            row = insert_condition(row=row, user_speaker_id=user_speaker_id, use_condition=use_condition)\n",
    "            assert row is not None, \"Row is None!\"\n",
    "            data.append(row)\n",
    "\n",
    "        init_path(output_dir + dir_postfix, reset=False)\n",
    "        with open(output_dir + dir_postfix + filename, \"w\", encoding=\"utf-8\") as fp:\n",
    "            json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><hr><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectStar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T14:30:29.398529Z",
     "start_time": "2021-09-10T14:30:29.392526Z"
    }
   },
   "outputs": [],
   "source": [
    "u_utterance = {\n",
    "    \"P\": \"즐거움\",\n",
    "    \"J\": \"기쁨\",\n",
    "    \"S\": \"슬픔\",\n",
    "    \"A\": \"분노\",\n",
    "    \"O\": \"보통\"\n",
    "}\n",
    "g_utterance = {\n",
    "    \"U\": \"유저정보\",\n",
    "    \"R\": \"페르소나\",\n",
    "    \"K\": \"외부지식\",\n",
    "    \"E\": \"공감형대화\",\n",
    "    \"N\": \"일반/기타\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T14:34:51.559381Z",
     "start_time": "2021-09-10T14:34:51.411387Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = \"SelectStar\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "extension = \"json\"\n",
    "\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "data = []\n",
    "for filename in os.listdir(raw_dir):\n",
    "    if not filename.endswith(extension): continue\n",
    "    if filename == 'SDS_final_2nd_20210901.json':  continue\n",
    "    with open(raw_dir+filename, \"r\", encoding=encoding) as fp:\n",
    "        __data = json.load(fp)\n",
    "        \n",
    "        _data = []\n",
    "        for row in __data:\n",
    "            if \"speakers_ids\" in row:\n",
    "                row[\"speaker_ids\"] = row.pop(\"speakers_ids\")\n",
    "            if isinstance(row[\"persona\"], dict) and \"g\" in row[\"persona\"] and \"u\" in row[\"persona\"]:\n",
    "                row[\"persona\"] = list(row[\"persona\"].values())\n",
    "            _data.append(row)\n",
    "        data += _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T14:34:59.574282Z",
     "start_time": "2021-09-10T14:34:59.243352Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3351/3351 [00:00<00:00, 10488.66it/s]\n"
     ]
    }
   ],
   "source": [
    "name_entity_pattern = \"\\$\\{[^(\\$\\{).]*\\}\"\n",
    "with open(raw_dir + \"SDS_final_2nd_20210901.json\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    _data = json.load(fp)\n",
    "    \n",
    "new_output = []\n",
    "for row_idx, row in enumerate(tqdm(_data, initial=0, total=len(_data))):\n",
    "    row = row[0]\n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "    entities = {\"name\": []}\n",
    "    for utterance, speaker_id in zip(row[\"utterances\"], row[\"speakers_ids\"]):\n",
    "        _utterances = [_utterance for _utterance in utterance.split(\"\\n\") if _utterance.strip()!=\"\"]\n",
    "        utterances += _utterances\n",
    "        speaker_ids += len(_utterances) * [speaker_id]\n",
    "    \n",
    "    labels = []\n",
    "    for utterance_idx, (utterance, speaker_id) in enumerate(zip(utterances, speaker_ids)):\n",
    "        counter_speaker_id = (speaker_id + 1) % 2\n",
    "        counter_persona = None\n",
    "        for k,v in row[\"persona\"].items():\n",
    "            if v[\"id\"] == counter_speaker_id:\n",
    "                counter_persona = v\n",
    "                break\n",
    "        \n",
    "        label = \"\"\n",
    "        search_result = re.search(name_entity_pattern, utterance)\n",
    "        while search_result is not None:\n",
    "            begin_idx = search_result.start()\n",
    "            end_idx = search_result.end()\n",
    "            name_to_replace = utterance[begin_idx+2:end_idx-1]\n",
    "            utterance = utterance[:begin_idx] + name_to_replace + utterance[end_idx:]\n",
    "            utterances[utterance_idx] = utterance\n",
    "            \n",
    "            if name_to_replace in counter_persona[\"name\"]:\n",
    "                end_idx = end_idx - 3\n",
    "                span = end_idx - begin_idx\n",
    "                entitiy_row = (utterance_idx, begin_idx, end_idx, span)\n",
    "                entities[\"name\"].append(entitiy_row)\n",
    "                label = \"U\"\n",
    "\n",
    "            # if speaker_id == 0: label = \"U\"\n",
    "            search_result = re.search(name_entity_pattern, utterance)\n",
    "        labels.append(label)\n",
    "    \n",
    "    row.pop(\"speakers_ids\")\n",
    "    row[\"idx\"] = row_idx\n",
    "    row[\"utterances\"] = utterances\n",
    "    row[\"speaker_ids\"] = speaker_ids\n",
    "    row[\"persona\"] = list(row[\"persona\"].values())\n",
    "    row[\"labels\"] = labels\n",
    "    row[\"entities\"] = entities\n",
    "    assert len(row[\"utterances\"])==len(row[\"speaker_ids\"])==len(row[\"labels\"]), \"length does not match\"\n",
    "    new_output.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T14:35:00.770335Z",
     "start_time": "2021-09-10T14:35:00.757332Z"
    }
   },
   "outputs": [],
   "source": [
    "data += new_output\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T14:35:02.321998Z",
     "start_time": "2021-09-10T14:35:01.738444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 5693\n",
      "error nums: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"data size:\", len(data))\n",
    "\n",
    "error_indice = []\n",
    "for row_idx, row in enumerate(data):\n",
    "    error_list = []\n",
    "    if len(row[\"persona\"])!=2: \n",
    "        error_list.append(\"persona length: {}\".format(len(row[\"persona\"])))\n",
    "    if len(row[\"utterances\"])!=len(row[\"speaker_ids\"]) or len(row[\"utterances\"])!=len(row[\"labels\"]) or len(row[\"speaker_ids\"])!=len(row[\"labels\"]): \n",
    "        error_list.append(\"length difference: {}/{}/{}\".format(len(row[\"utterances\"]),len(row[\"speaker_ids\"]),len(row[\"labels\"])))\n",
    "\n",
    "#     for speaker_id, label in zip(row[\"speaker_ids\"], row[\"labels\"]):\n",
    "#         if speaker_id == 0 and label not in g_utterance:\n",
    "#             error_list.append(\"wrong label for g: '{}'\".format(label))\n",
    "#         if speaker_id == 1 and label not in u_utterance:\n",
    "#             error_list.append(\"wrong label for u: '{}'\".format(label))\n",
    "\n",
    "    if len(error_list) > 0:\n",
    "        error_indice.append((row_idx, error_list))\n",
    "        print(\"error: {}\".format(row_idx))\n",
    "        \n",
    "string_output = \"\"\n",
    "for row in error_indice:\n",
    "    _str = \"{}:\\n\".format(row[0])\n",
    "    for error in row[1]:\n",
    "        _str += \"\\t{}\\n\".format(error)\n",
    "    string_output += _str\n",
    "\n",
    "print(\"error nums:\", len(error_indice))\n",
    "if len(error_indice) <= 0:\n",
    "#     random.shuffle(output)\n",
    "    with open(root_dir + \"/{language}/multi_turn/feed_data_0.json\".format(language=language), \"w\", encoding=encoding) as fp:\n",
    "        json.dump(data, fp)\n",
    "else:\n",
    "    with open(\"./error_contents.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(string_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIHUB_SSGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"AIHUB_SSGI\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "final_output = []\n",
    "file_name_list = [file_name for file_name in os.listdir(raw_dir+\"dialog\") if file_name.endswith(\".xlsx\")]\n",
    "for file_name in file_name_list:\n",
    "    df = pd.read_excel(raw_dir+\"dialog/\"+file_name, engine=\"openpyxl\")\n",
    "    df = df.fillna(\"\")\n",
    "    \n",
    "    speaker_info = dict()\n",
    "    domain = None\n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "    categories = []\n",
    "    tasks = []\n",
    "    subtasks = []\n",
    "\n",
    "    idx = 0\n",
    "    output = []\n",
    "    for row in df[[\"SPEAKER\", \"SENTENCE\", \"DOMAIN\", \"CATEGORY\", \"MAIN\", \"SUB\", \"SPEAKERID\", \"SENTENCEID\"]].values.tolist():\n",
    "        speaker, sentence, _domain, category, main, sub, speaker_id, sentence_id = row\n",
    "\n",
    "        if int(sentence_id) == 1 and len(utterances) > 0:\n",
    "            # append\n",
    "            output_row = OrderedDict()\n",
    "            output_row[\"idx\"] = idx\n",
    "            output_row[\"speakers\"] = dict(OrderedDict(sorted(speaker_info.items())))\n",
    "            output_row[\"domain\"] = domain\n",
    "            output_row[\"utterances\"] = utterances\n",
    "            output_row[\"speaker_ids\"] = speaker_ids\n",
    "            output_row[\"category\"] = trim_obj(categories)\n",
    "            output_row[\"task\"] = trim_obj(tasks)\n",
    "            output_row[\"subtask\"] = trim_obj(subtasks)\n",
    "            output_row = dict(output_row)\n",
    "            output.append(output_row)\n",
    "\n",
    "            # reset\n",
    "            idx += 1\n",
    "            speaker_info = dict()\n",
    "            domain = None\n",
    "            utterances = []\n",
    "            speaker_ids = []\n",
    "            categories = []\n",
    "            tasks = []\n",
    "            subtasks = []\n",
    "\n",
    "        speaker_info[speaker_id] = speaker\n",
    "        domain = _domain\n",
    "        utterances.append(sentence)\n",
    "        speaker_ids.append(speaker_id)\n",
    "        categories.append(category)\n",
    "        tasks.append(main)\n",
    "        subtasks.append(sub)\n",
    "\n",
    "    # append\n",
    "    output_row = OrderedDict()\n",
    "    output_row[\"idx\"] = idx\n",
    "    output_row[\"speakers\"] = speaker_info\n",
    "    output_row[\"domain\"] = domain\n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row[\"category\"] = categories\n",
    "    output_row[\"task\"] = tasks\n",
    "    output_row[\"subtask\"] = subtasks\n",
    "    output_row = dict(output_row)\n",
    "    output.append(output_row)\n",
    "\n",
    "    final_output += output\n",
    "    print(\"output_size:\", len(output), \"final_output size:\", len(final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIHUB_Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"AIHUB_Twitter\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "df = pd.read_excel(raw_dir+\"트위터_대화시나리오DB_2000Set.xlsx\", engine=\"openpyxl\")\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "output = []\n",
    "for row in df.values.tolist():\n",
    "    utterances = []\n",
    "    for utterance in row:\n",
    "        if utterance == \"\": break\n",
    "        utterances.append(utterance)\n",
    "    speaker_ids = [i%2 for i in range(0, len(utterances))]\n",
    "    \n",
    "    output_row = OrderedDict()\n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row = dict(output_row)\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIHUB_EMOTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "extension = \"xlsx\"\n",
    "\n",
    "dataset_name = \"AIHUB_EMOTION\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for file_name in os.listdir(raw_dir):\n",
    "    if not file_name.endswith(extension): continue\n",
    "    _df = pd.read_excel(raw_dir+file_name)\n",
    "    df = pd.concat([df, _df], axis=0)\n",
    "df = df.fillna(\"\")\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "output = []\n",
    "domain = \"emotional_dialog\"\n",
    "for row_idx, row in enumerate(df.values.tolist()):\n",
    "    _, _, age_range, gender, category, healt_status, task, subtask, u1, s1, u2, s2, u3, s3 = row\n",
    "    if gender == \"여성\": gender = \"F\"\n",
    "    elif gender == \"남성\": gender = \"M\"\n",
    "    else: gender = \"O\"\n",
    "    speakers = {\"0\":\"시스템\", \"1\":\"사용자\"}\n",
    "    utterances = [u1, s1, u2, s2, u3, s3]\n",
    "    utterances = [utterance for utterance in utterances if utterance!=\"\"]\n",
    "    speaker_ids = [(i+1)%2 for i in range(0, len(utterances))]\n",
    "    \n",
    "    output_row = OrderedDict()\n",
    "    output_row[\"speakers\"] = speakers\n",
    "    output_row[\"domain\"] = domain\n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row[\"category\"] = category\n",
    "    output_row[\"task\"] = task\n",
    "    output_row[\"subtask\"] = subtask\n",
    "    output_row = dict(output_row)\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIHUB_SNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "extension = \"json\"\n",
    "\n",
    "dataset_name = \"AIHUB_SNS\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "data = []\n",
    "for file_name in os.listdir(raw_dir):\n",
    "    if not file_name.endswith(extension): continue\n",
    "    with open(raw_dir+file_name, \"r\", encoding=encoding) as fp:\n",
    "        _data = json.load(fp)\n",
    "        data += _data[\"data\"]\n",
    "    break\n",
    "\n",
    "output = []\n",
    "for row_idx, row in enumerate(data):\n",
    "    domain = row[\"header\"][\"dialogueInfo\"][\"type\"]\n",
    "    category = row[\"header\"][\"dialogueInfo\"][\"topic\"]\n",
    "    speakers = OrderedDict()\n",
    "    speaker_mapping = dict()\n",
    "    for speaker_id, speaker in enumerate(row[\"header\"][\"participantsInfo\"]):\n",
    "        speaker_mapping[speaker[\"participantID\"]] = speaker_id\n",
    "        speakers[speaker_id] = OrderedDict()\n",
    "        speakers[speaker_id][\"id\"] = speaker[\"participantID\"]\n",
    "        speakers[speaker_id][\"sex\"] = speaker[\"gender\"]\n",
    "        speakers[speaker_id][\"age\"] = speaker[\"age\"]\n",
    "        speakers[speaker_id][\"residence\"] = speaker[\"residentialProvince\"]\n",
    "        speakers[speaker_id] = dict(speakers[speaker_id])\n",
    "    speakers = dict(speakers)\n",
    "    summary = row[\"body\"][\"summary\"]\n",
    "    \n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "    datetime_list = []\n",
    "    for _row in row[\"body\"][\"dialogue\"]:\n",
    "        utterance = _row[\"utterance\"]\n",
    "        utterances.append(utterance)\n",
    "        speaker_id = speaker_mapping[_row[\"participantID\"]]\n",
    "        speaker_ids.append(speaker_id)\n",
    "        _datetime = _row[\"date\"] + \" \" + _row[\"time\"]\n",
    "        datetime_list.append(_datetime)\n",
    "    \n",
    "    output_row = OrderedDict()\n",
    "    output_row[\"idx\"] = row_idx\n",
    "    output_row[\"speakers\"] = speakers\n",
    "    output_row[\"domain\"] = domain\n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row[\"datetime\"] = datetime_list\n",
    "    output_row[\"category\"] = category\n",
    "    output_row[\"summary\"] = summary\n",
    "    output_row = dict(output_row)\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KLUE_DST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"KLUE_DST\" # 국립국어원\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "file_name_list = [file_name for file_name in os.listdir(raw_dir) if file_name.endswith(\".json\") and file_name.startswith(\"wos\")]\n",
    "\n",
    "output = []\n",
    "kor_regex = \"[ㄱ-ㅣ가-힣]\"\n",
    "\n",
    "for file_name in file_name_list:\n",
    "    with open(raw_dir+file_name, \"r\", encoding=encoding) as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    for dialog_idx, dialog_row in enumerate(data):\n",
    "        dialog = dialog_row[\"dialogue\"]\n",
    "\n",
    "        output_row = OrderedDict()\n",
    "        utterances = []\n",
    "        speaker_ids = []\n",
    "        category = []\n",
    "        task = []\n",
    "        subtask = []\n",
    "        _speaker_info = dict()\n",
    "        for row in dialog:\n",
    "            speaker = row[\"role\"]\n",
    "            if speaker not in _speaker_info:\n",
    "                _speaker_info[speaker] = len(_speaker_info)\n",
    "            utterance = row[\"text\"]\n",
    "            utterances.append(utterance)\n",
    "            speaker_id = _speaker_info[speaker]\n",
    "            speaker_ids.append(speaker_id)\n",
    "\n",
    "            if \"state\" not in row: continue\n",
    "            state = row[\"state\"]\n",
    "            for _state in state:\n",
    "                _state = _state.split(\"-\")\n",
    "                if len(_state) > 0: \n",
    "                    _category = _state[0]\n",
    "                    category.append(_category)\n",
    "                if len(_state) > 1:\n",
    "                    _task = _state[1]\n",
    "                    task.append(_task)\n",
    "                if len(_state) > 2:\n",
    "                    _subtask = _state[2]\n",
    "                    if re.search(kor_regex, _subtask) is not None:\n",
    "                        subtask.append(_subtask)\n",
    "            category = trim_obj(category)\n",
    "            task = trim_obj(task)\n",
    "            subtask = trim_obj(subtask)\n",
    "\n",
    "        speaker_info = {v:k for k,v in _speaker_info.items()}\n",
    "\n",
    "        output_row[\"idx\"] = dialog_idx\n",
    "        output_row[\"speakers\"] = speaker_info\n",
    "        output_row[\"utterances\"] = utterances\n",
    "        output_row[\"speaker_ids\"] = speaker_ids\n",
    "        output_row[\"category\"] = category\n",
    "        output_row[\"task\"] = task\n",
    "        output_row[\"subtask\"] = subtask\n",
    "        output_row = dict(output_row)\n",
    "        output.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIKL_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"NIKL_2020\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "file_name_list = [file_name for file_name in os.listdir(raw_dir) if file_name.endswith(\".json\")]\n",
    "output = []\n",
    "for idx, file_name in enumerate(file_name_list):\n",
    "    with open(raw_dir+file_name, \"r\", encoding=encoding) as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    # date\n",
    "    # date = datetime.strptime(data[\"document\"][0][\"metadata\"][\"date\"], \"%Y%M%d\").strftime(\"%Y-%M-%d\")\n",
    "    _date = data[\"document\"][0][\"metadata\"][\"date\"]\n",
    "    date = \"-\".join([_date[0:4], _date[4:6], _date[6:8]])\n",
    "    # tasks\n",
    "    tasks = data[\"document\"][0][\"metadata\"][\"topic\"]\n",
    "    tasks = tasks.split(\">\")[-1]\n",
    "    tasks = [task.strip() for task in tasks.split(\",\")]\n",
    "    tasks = trim_obj(tasks)\n",
    "    # speaker_info\n",
    "    spkid2id = {speaker[\"id\"]:_idx for _idx, speaker in enumerate(data[\"document\"][0][\"metadata\"][\"speaker\"])}\n",
    "    speaker_info = {spkid2id[speaker[\"id\"]]:speaker for speaker in data[\"document\"][0][\"metadata\"][\"speaker\"]}\n",
    "    speaker_info = dict(OrderedDict(sorted(speaker_info.items())))\n",
    "    # utterances & speaker_ids\n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "    elements = data[\"document\"][0][\"utterance\"]\n",
    "    prev_utterance = elements[0][\"form\"] # 철자 전사\n",
    "    # utterance = elements[0][\"original_form\"] # 발음 전사\n",
    "    prev_speaker_id = elements[0][\"speaker_id\"]\n",
    "    for element_idx in range(1, len(elements)):\n",
    "        cur_utterance = elements[element_idx][\"form\"] # 철자 전사\n",
    "        # cur_utterance = elements[element_idx][\"original_form\"] # 발음 전사\n",
    "        cur_speaker_id = elements[element_idx][\"speaker_id\"]\n",
    "\n",
    "        if cur_speaker_id == prev_speaker_id:\n",
    "            if prev_utterance.endswith(\".\"): \n",
    "                utterances.append(prev_utterance)\n",
    "                speaker_ids.append(prev_speaker_id)\n",
    "                prev_utterance = cur_utterance\n",
    "            else:\n",
    "                prev_utterance = prev_utterance + cur_utterance\n",
    "        else:\n",
    "            for _prev_utterance in prev_utterance.split(\". \"):\n",
    "                if _prev_utterance.strip() == \"\": continue\n",
    "                if prev_utterance.strip().endswith(\".\") and not _prev_utterance.strip().endswith(\".\"): \n",
    "                    _prev_utterance = _prev_utterance + \".\"\n",
    "                utterances.append(_prev_utterance)\n",
    "                speaker_ids.append(prev_speaker_id)\n",
    "            # reset\n",
    "            prev_utterance = cur_utterance\n",
    "            prev_speaker_id = cur_speaker_id\n",
    "    utterances.append(prev_utterance)\n",
    "    speaker_ids.append(prev_speaker_id)\n",
    "    speaker_ids = [spkid2id[speaker_id] for speaker_id in speaker_ids]\n",
    "\n",
    "\n",
    "    output_row = OrderedDict()\n",
    "    output_row[\"idx\"] = idx\n",
    "    output_row[\"document_id\"] = data[\"document\"][0][\"id\"]\n",
    "    output_row[\"date\"] = date\n",
    "    output_row[\"task\"] = tasks\n",
    "    output_row[\"speaker_info\"] = speaker_info\n",
    "    output_row[\"setting\"] = data[\"document\"][0][\"metadata\"][\"setting\"]\n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row = dict(output_row)\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIKL_Dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_row(idx, soup):\n",
    "    output_row = OrderedDict()\n",
    "    speaker_info = dict()\n",
    "    _speaker_id_mapping = dict()\n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "\n",
    "    header = soup.find(\"body\").find(\"teiheader\")\n",
    "    _file_desc = header.find(\"filedesc\")\n",
    "    _profile_desc = header.find(\"profiledesc\")\n",
    "    speakers = _profile_desc.findAll(\"person\")\n",
    "    for speaker_id, speaker in enumerate(speakers):\n",
    "        _speaker_info = OrderedDict(speaker.attrs)\n",
    "        if len(splited) > 0:\n",
    "            _speaker_info[\"job\"] = splited[0].strip()\n",
    "        if len(splited) > 1:\n",
    "            _speaker_info[\"residence\"] = splited[1].strip()\n",
    "        speaker_info[speaker_id] = dict(_speaker_info)\n",
    "        _speaker_id_mapping[_speaker_info[\"id\"]] = speaker_id\n",
    "\n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "\n",
    "    prev_speaker_id = -1\n",
    "    prev_utterance = \"\"\n",
    "    _utterances = soup.findAll(\"u\")\n",
    "    for _utterance in _utterances:\n",
    "        _speaker_id = _utterance.attrs[\"who\"]\n",
    "        if _speaker_id not in _speaker_id_mapping: continue\n",
    "        speaker_id = _speaker_id_mapping[_speaker_id]\n",
    "        # utterance = [u.text.strip() for u in _utterance.findAll(\"s\")]\n",
    "        utterance = [child.strip() for s in _utterance.findAll(\"s\") for child in s.children if isinstance(child, str)]\n",
    "        utterance = \" \".join(utterance)\n",
    "        utterance = utterance.replace(\"::\", \"\")\n",
    "        if utterance == \"\": continue\n",
    "\n",
    "        if speaker_id == prev_speaker_id:\n",
    "            prev_utterance = prev_utterance + \" \" + utterance\n",
    "        else:\n",
    "            if prev_speaker_id != -1:\n",
    "                utterances.append(prev_utterance)\n",
    "                speaker_ids.append(prev_speaker_id)\n",
    "            prev_utterance = utterance\n",
    "            prev_speaker_id = speaker_id\n",
    "\n",
    "    if speaker_id == prev_speaker_id:\n",
    "        prev_utterance = prev_utterance + utterance\n",
    "    utterances.append(prev_utterance)\n",
    "    speaker_ids.append(prev_speaker_id)\n",
    "    \n",
    "    assert len(utterances) == len(speaker_ids), \"{l1} vs {l2}\".format(l1=len(utterances), l2=len(speaker_ids))\n",
    "\n",
    "    output_row[\"idx\"] = idx\n",
    "    output_row[\"speakers\"] = speaker_info\n",
    "    output_row[\"title\"] = _file_desc.find(\"title\").text\n",
    "    output_row[\"domain\"] = _profile_desc.find(\"settingdesc\").text\n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row[\"project_name\"] = header.find(\"projectdesc\").text\n",
    "    output_row[\"distributor\"] = _file_desc.find(\"distributor\").text\n",
    "    extent = _file_desc.find(\"extent\").text\n",
    "    output_row[\"num_eojeols\"] = int(re.sub(\"[^0-9]\", \"\", extent))\n",
    "    output_row = dict(output_row)\n",
    "    return output_row\n",
    "\n",
    "file_path_list = []\n",
    "for sub_dir in os.listdir(raw_dir):\n",
    "    file_path = raw_dir+sub_dir+\"/원시/\"\n",
    "    if not os.path.isdir(file_path): continue\n",
    "    file_name_list = os.listdir(file_path)\n",
    "    if len(file_name_list) != 1: continue\n",
    "    file_name = file_name_list[0]\n",
    "    file_path_list.append(file_path+file_name)\n",
    "    \n",
    "output = []\n",
    "for idx, file_path in enumerate(file_path_list):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        contents = f.read()\n",
    "    contents = contents.decode(\"utf-16\")\n",
    "    contents = re.sub(\"\\r\\n\", \"\", contents)\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "    output_row = get_output_row(idx=idx, soup=soup)\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSubtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"OpenSubtitles\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "pickle_path = file_path[dataset_name][\"pickle\"].format(root_dir=root_dir, language=language)\n",
    "data = None\n",
    "with open(pickle_path, \"rb\") as fp:\n",
    "    data = pickle.load(fp)\n",
    "\n",
    "output = []\n",
    "kor_regex = \"[ㄱ-ㅣ가-힣]\"\n",
    "\n",
    "_data = data[\"train\"] + data[\"test\"]\n",
    "idx = 0\n",
    "for row in _data:\n",
    "    output_row = OrderedDict()\n",
    "    output_row[\"idx\"] = idx\n",
    "    output_row[\"file_id\"] = row.pop(\"file_id\")\n",
    "    context = row.pop(\"context\")\n",
    "    response = row.pop(\"response\")\n",
    "    \n",
    "    _utterances = [row[k] for k in sorted(row.keys(), reverse=True)]\n",
    "    _utterances.append(context)\n",
    "    _utterances.append(response)\n",
    "\n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "    _idx = 0\n",
    "    \n",
    "    not_korean = True\n",
    "    for utterance in _utterances:\n",
    "        speaker_id = -1 * ((_idx+1) % 2)\n",
    "        _idx += 1\n",
    "        utterance = utterance.replace(\"'\", \"\")\n",
    "        utterance = utterance.replace('\"', '')\n",
    "        utterance = re.sub(\"^.* :\", \"\", utterance).strip()\n",
    "        utterance = utterance.replace(\"nbsp;\", \"\").strip()\n",
    "        if utterance==\"\": continue\n",
    "        utterances.append(utterance)\n",
    "        speaker_ids.append(speaker_id)\n",
    "        if re.search(kor_regex, utterance) is not None: not_korean = False\n",
    "    \n",
    "    if not_korean: continue\n",
    "        \n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row = dict(output_row)\n",
    "    output.append(output_row)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmpatheticDialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 2342\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"SelectStar\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "extension = \"json\"\n",
    "\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "data = []\n",
    "for filename in os.listdir(raw_dir):\n",
    "    if not filename.endswith(extension): continue\n",
    "    with open(raw_dir+filename, \"r\", encoding=encoding) as fp:\n",
    "        __data = json.load(fp)\n",
    "        \n",
    "        _data = []\n",
    "        for row in __data:\n",
    "            if \"speakers_ids\" in row:\n",
    "                row[\"speaker_ids\"] = row.pop(\"speakers_ids\")\n",
    "            _data.append(row)\n",
    "        data += _data\n",
    "print(\"data size:\", len(data))\n",
    "total_personas = [persona for row in data for persona in row[\"persona\"].values()]\n",
    "persona_df = pd.DataFrame(total_personas)\n",
    "persona_df = persona_df.fillna(\"\")\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "def find_most_similar_persona(age, gender, default_name=\"당신\"):\n",
    "    target_df = persona_df\n",
    "    target_persona = {\"name\": default_name}\n",
    "    try:\n",
    "        target_df = target_df[target_df[\"gender\"]==gender]\n",
    "        age_diff = abs(target_df[\"age\"] - age)\n",
    "        target_df[\"age_diff\"] = list(age_diff)\n",
    "        target_age_diff = target_df.sort_values(\"age_diff\").values.tolist()[0][-1]\n",
    "        target_df = target_df.loc[(target_df[\"age_diff\"]==target_age_diff)]\n",
    "        target_persona = target_df.sample(1).to_dict(\"records\")[0]\n",
    "    except:\n",
    "        target_persona = target_df.sample(1).to_dict(\"records\")[0]\n",
    "    return target_persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"EmpatheticDialogues\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "extension = \"xlsx\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "filename_list = os.listdir(raw_dir)\n",
    "df = pd.DataFrame()\n",
    "for filename in filename_list:\n",
    "    if not filename.endswith(extension): continue\n",
    "    _df = pd.read_excel(raw_dir + filename)\n",
    "    df = pd.concat([df, _df], axis=0)\n",
    "df = df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make preprocessed raw_data\n",
    "data = []\n",
    "\n",
    "name_entity_pattern = \"\\$\\$[ㄱ-ㅣ가-힣]+\\$\\$\"\n",
    "prev_row_idx = -1\n",
    "utterances = []\n",
    "speaker_ids = []\n",
    "personas = []\n",
    "entities = dict()\n",
    "turn_idx = 0\n",
    "\n",
    "for _idx, row in enumerate(df.values.tolist()):    \n",
    "    row_idx, persona_idx, eng, kor_before, kor_after, gender, age, married, job = row\n",
    "    \n",
    "    if _idx == 0: \n",
    "        prev_row_idx = row_idx\n",
    "    if _idx > 0 and row_idx != \"\" and prev_row_idx != int(row_idx):\n",
    "        # append dialog\n",
    "        output_row = OrderedDict()\n",
    "        output_row[\"idx\"] = int(prev_row_idx)\n",
    "        output_row[\"utterances\"] = utterances\n",
    "        output_row[\"speaker_ids\"] = speaker_ids\n",
    "        output_row[\"persona\"] = personas\n",
    "        output_row[\"entities\"] = entities\n",
    "        output_row = dict(output_row)\n",
    "        data.append(output_row)\n",
    "        # reset\n",
    "        prev_row_idx = row_idx\n",
    "        utterances = []\n",
    "        speaker_ids = []\n",
    "        personas = []\n",
    "        entities = dict()\n",
    "        turn_idx = 0\n",
    "        \n",
    "    _utterances = clean(str(kor_after))\n",
    "    speaker_id = (turn_idx+1) % 2\n",
    "    for utterance in _utterances.split(\". \"):\n",
    "        utterances.append(utterance)\n",
    "        speaker_ids.append(speaker_id)\n",
    "        \n",
    "    if gender!=\"\":\n",
    "        persona = OrderedDict()\n",
    "        persona[\"id\"] = speaker_id\n",
    "        persona[\"persona_idx\"] = int(persona_idx)\n",
    "        persona[\"gender\"] = gender\n",
    "        if age != \"\": \n",
    "            persona[\"age\"] = int(age)\n",
    "        if married != \"\": \n",
    "            persona[\"married\"] = int(married)\n",
    "        if job != \"\": \n",
    "            persona[\"job\"] = int(job)\n",
    "        persona = dict(persona)\n",
    "        personas.append(persona)\n",
    "    turn_idx += 1\n",
    "    \n",
    "# append dialog\n",
    "output_row = OrderedDict()\n",
    "output_row[\"idx\"] = int(prev_row_idx)\n",
    "output_row[\"utterances\"] = utterances\n",
    "output_row[\"speaker_ids\"] = speaker_ids\n",
    "output_row[\"persona\"] = personas\n",
    "output_row[\"entities\"] = entities\n",
    "output_row = dict(output_row)\n",
    "data.append(output_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이름 채워넣기\n",
    "_data = data.copy()\n",
    "data = []\n",
    "for row in _data: \n",
    "    new_personas = []\n",
    "    for persona in row[\"persona\"]:\n",
    "        new_persona = find_most_similar_persona(age=persona[\"age\"], gender=persona[\"gender\"])\n",
    "        persona[\"name\"] = new_persona[\"name\"]\n",
    "        new_personas.append(persona)\n",
    "    row[\"persona\"] = new_personas\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이름 치환 & entities 정보 수집\n",
    "name_entity_pattern = \"\\$\\$[ㄱ-ㅣ가-힣]+\\$\\$\"\n",
    "\n",
    "_data = data.copy()\n",
    "data = []\n",
    "\n",
    "for _row in _data:\n",
    "    row = _row.copy()\n",
    "    \n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "    labels = []\n",
    "    entities = dict()\n",
    "    for utterance_idx, (_utterance, _speaker_id) in enumerate(zip(row[\"utterances\"], row[\"speaker_ids\"])):\n",
    "        label = \"\" if _speaker_id == 1 else \"N\"\n",
    "        \n",
    "        while True:\n",
    "            name_entity_search = re.search(name_entity_pattern, _utterance)\n",
    "            if name_entity_search is not None:\n",
    "                cur_persona = [persona for persona in row[\"persona\"] if persona[\"id\"]==(_speaker_id+1)%2][0]\n",
    "                entity_start_idx = name_entity_search.start()\n",
    "                entity_end_idx = name_entity_search.end()\n",
    "\n",
    "                name = cur_persona[\"name\"]\n",
    "                if len(name) == 3: name = name[1:]\n",
    "                _utterance = _utterance[:entity_start_idx] + name + _utterance[entity_end_idx:]\n",
    "                entity_span = len(name)\n",
    "                entity_end_idx = entity_start_idx + entity_span\n",
    "\n",
    "                entity = [utterance_idx, entity_start_idx, entity_end_idx, entity_span]\n",
    "                if \"name\" not in entities: entities[\"name\"] = []\n",
    "                entities[\"name\"].append(entity)\n",
    "                if _speaker_id == 0: label = \"U\"\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        utterances.append(_utterance)\n",
    "        speaker_ids.append(_speaker_id)\n",
    "        labels.append(label)\n",
    "\n",
    "    row[\"utterances\"] = utterances\n",
    "    row[\"speaker_ids\"] = speaker_ids\n",
    "    row[\"labels\"] = labels\n",
    "    row[\"entities\"] = entities\n",
    "    \n",
    "    data.append(row)\n",
    "\n",
    "# with open(raw_dir + \"empathetic_dialogues.pickle\", \"wb\") as fp:\n",
    "#     pickle.dump(data, fp)\n",
    "# with open(root_dir + \"/{language}/multi_turn/feed_data_0.json\".format(language=language), \"w\", encoding=encoding) as fp:\n",
    "#     json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"EmpatheticDialogues\"\n",
    "language = \"eng\"\n",
    "encoding = \"UTF-8\"\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "_data = None\n",
    "with open(raw_dir+\"empathetic_dialogues.pickle\", \"rb\") as fp:\n",
    "    _data = pickle.load(fp)\n",
    "\n",
    "output = []\n",
    "row_idx = 0\n",
    "for k,v in _data.items():\n",
    "    print(\"v size:\", len(v))\n",
    "    print(\"v conv size:\", len(pd.DataFrame(v)[\"conv_id\"].unique()))\n",
    "    rows = []\n",
    "\n",
    "    prev_conv_id = None\n",
    "    output_row = OrderedDict()\n",
    "    for row in v:\n",
    "        cur_conv_id = row[\"conv_id\"]\n",
    "        if prev_conv_id is None:\n",
    "            output_row[\"idx\"] = row_idx\n",
    "            output_row[\"context\"] = row[\"context\"]\n",
    "            output_row[\"prompt\"] = row[\"prompt\"]\n",
    "            output_row[\"utterances\"] = []\n",
    "            output_row[\"speaker_ids\"] = []\n",
    "            output_row[\"selfeval\"] = row[\"selfeval\"]\n",
    "            output_row[\"tags\"] = []\n",
    "            prev_conv_id = cur_conv_id\n",
    "            continue\n",
    "\n",
    "        if cur_conv_id != prev_conv_id:\n",
    "            tags = sorted(set(output_row[\"tags\"]))\n",
    "            output_row[\"tags\"] = tags\n",
    "            prompt = output_row.pop(\"prompt\")\n",
    "            output_row[\"utterances\"].insert(0, prompt)\n",
    "            if len(output_row[\"speaker_ids\"]) == 0:\n",
    "                previous_speaker_id = 0\n",
    "            elif len(output_row[\"speaker_ids\"]) > 1:\n",
    "                previous_speaker_id = output_row[\"speaker_ids\"][1]\n",
    "            else:\n",
    "                if output_row[\"speaker_ids\"][0] == 0:\n",
    "                    previous_speaker_id = 1\n",
    "                else:\n",
    "                    previous_speaker_id = 0\n",
    "            output_row[\"speaker_ids\"].insert(0, previous_speaker_id)\n",
    "            output_row = dict(output_row)\n",
    "            rows.append(output_row)\n",
    "            prev_conv_id = cur_conv_id\n",
    "            row_idx += 1\n",
    "\n",
    "            output_row = OrderedDict()\n",
    "            output_row[\"idx\"] = row_idx\n",
    "            output_row[\"context\"] = row[\"context\"]\n",
    "            output_row[\"prompt\"] = row[\"prompt\"]\n",
    "            output_row[\"utterances\"] = []\n",
    "            output_row[\"speaker_ids\"] = []\n",
    "            output_row[\"selfeval\"] = row[\"selfeval\"]\n",
    "            output_row[\"tags\"] = []\n",
    "        else:\n",
    "            output_row[\"utterances\"].append(row[\"utterance\"])\n",
    "            output_row[\"speaker_ids\"].append(int(row[\"speaker_idx\"]))\n",
    "            output_row[\"tags\"].append(row[\"tags\"])\n",
    "\n",
    "    tags = sorted(set(output_row[\"tags\"]))\n",
    "    output_row[\"tags\"] = tags\n",
    "    prompt = output_row.pop(\"prompt\")\n",
    "    output_row[\"utterances\"].insert(0, prompt)\n",
    "    if len(output_row[\"speaker_ids\"]) > 1:\n",
    "        previous_speaker_id = output_row[\"speaker_ids\"][1]\n",
    "    else:\n",
    "        if output_row[\"speaker_ids\"][0] == 0:\n",
    "            previous_speaker_id = 1\n",
    "        else:\n",
    "            previous_speaker_id = 0\n",
    "    output_row[\"speaker_ids\"].insert(0, previous_speaker_id)\n",
    "    output_row = dict(output_row)\n",
    "    rows.append(output_row)\n",
    "    prev_conv_id = cur_conv_id\n",
    "    row_idx += 1\n",
    "    \n",
    "    print(\"rows size:\", len(rows), \"\\n\")\n",
    "    output += rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KaggleConversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"KaggleConversation\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "titles = pd.read_csv(raw_dir + \"conversation_titles.csv\")\n",
    "data_df = pd.read_csv(raw_dir + \"conversations.csv\")\n",
    "\n",
    "output = []\n",
    "for row_idx, _date in enumerate(titles[\"date\"].unique().tolist()):\n",
    "    title = titles[titles[\"date\"]==_date][\"kor_title\"].tolist()[0]\n",
    "    translated_title = titles[titles[\"date\"]==_date][\"eng_title\"].tolist()[0]\n",
    "    utterances = data_df[data_df[\"date\"]==_date][\"kor_sent\"].tolist()\n",
    "    translated_utterances = data_df[data_df[\"date\"]==_date][\"eng_sent\"].tolist()\n",
    "    speaker_ids = [(i+1)%2 for i in range(0, len(utterances))]\n",
    "    \n",
    "    output_row = OrderedDict()\n",
    "    output_row[\"idx\"] = row_idx\n",
    "    output_row[\"title\"] = title\n",
    "    output_row[\"utterances\"] = utterances\n",
    "    output_row[\"speaker_ids\"] = speaker_ids\n",
    "    output_row[\"translated_title\"] = translated_title\n",
    "    output_row[\"translated_utterances\"] = translated_utterances\n",
    "    output_row = dict(output_row)\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectStarPersona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T14:27:32.453571Z",
     "start_time": "2021-09-10T14:27:32.422548Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = \"SelectStarPersona\"\n",
    "language = \"kor\"\n",
    "encoding = \"UTF-8\"\n",
    "name_entity_pattern = \"\\$\\{[^(\\$\\{).]*\\}\"\n",
    "\n",
    "root_dir = file_path[dataset_name][\"root_dir\"].format(dataset_dir=dataset_dir)\n",
    "raw_dir = file_path[dataset_name][\"raw_dir\"].format(root_dir=root_dir, language=language)\n",
    "\n",
    "file_name_list = [file_name for file_name in os.listdir(raw_dir) if file_name.endswith(\".json\")]\n",
    "_output = []\n",
    "for idx, file_name in enumerate(file_name_list):\n",
    "    with open(raw_dir+file_name, \"r\", encoding=encoding) as fp:\n",
    "        data = json.load(fp)\n",
    "        for rows in data:\n",
    "            for row in rows:\n",
    "                _output.append(row)\n",
    "                \n",
    "output = []\n",
    "for row in _output:\n",
    "    row[\"speaker_ids\"] = row.pop(\"speakers_ids\")\n",
    "\n",
    "    utterances = []\n",
    "    speaker_ids = []\n",
    "    for speaker_id, utterance in zip(row[\"speaker_ids\"], row[\"utterances\"]):\n",
    "        for _utterance in utterance.strip().split(\"\\n\"):\n",
    "            utterances.append(_utterance)\n",
    "            speaker_ids.append(speaker_id)\n",
    "\n",
    "    _utterances = []\n",
    "    _labels = []\n",
    "    entities = dict()\n",
    "    entities[\"name\"] = []\n",
    "\n",
    "    for _utterance_idx, (_utterance, _speaker_id) in enumerate(zip(utterances, speaker_ids)):\n",
    "        _label = \"\"\n",
    "        search_result = re.search(name_entity_pattern, _utterance)\n",
    "        while search_result is not None:\n",
    "            begin_idx = search_result.start()\n",
    "            end_idx = search_result.end()\n",
    "            _utterance = _utterance[:begin_idx] + _utterance[begin_idx+2:end_idx-1] + _utterance[end_idx:]\n",
    "\n",
    "            end_idx = end_idx - 3\n",
    "            span = end_idx - begin_idx\n",
    "            entitiy_row = (_utterance_idx, begin_idx, end_idx, span)\n",
    "            entities[\"name\"].append(entitiy_row)\n",
    "            if _speaker_id == 0: _label = \"U\"            \n",
    "\n",
    "            search_result = re.search(name_entity_pattern, _utterance)\n",
    "        _utterances.append(_utterance)\n",
    "        _labels.append(_label)\n",
    "\n",
    "    row[\"utterances\"] = _utterances\n",
    "    row[\"speaker_ids\"] = speaker_ids\n",
    "    row[\"entities\"] = entities\n",
    "    row[\"labels\"] = _labels\n",
    "    persona = list(row[\"persona\"].values())\n",
    "    row[\"persona\"] = persona\n",
    "    output.append(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
