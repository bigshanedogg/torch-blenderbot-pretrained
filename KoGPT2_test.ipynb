{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b66a8b7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T02:14:30.135574Z",
     "start_time": "2021-08-31T02:14:26.942912Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import subprocess\n",
    "# import gluonnlp\n",
    "import torch\n",
    "from torch.utils.data import DataLoader \n",
    "from gluonnlp.data import SentencepieceTokenizer \n",
    "\n",
    "# tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\")\n",
    "# https://hipgyung.tistory.com/entry/%ED%95%9C%EA%B5%AD%EC%96%B4-%EA%B0%80%EC%82%AC-%EB%8D%B0%EC%9D%B4%ED%84%B0-KoGPT2-Fine-Tuning\n",
    "# https://github.com/gyunggyung/KoGPT2-FineTuning\n",
    "# https://ratsgo.github.io/nlpbook/docs/generation/inference2/\n",
    "\n",
    "# git clone https://github.com/SKT-AI/KoGPT2.git\n",
    "# cd KoGPT2\n",
    "# pip install -r requirements.txt\n",
    "# pip install .\n",
    "# from kogpt2.utils import get_tokenizer\n",
    "# from kogpt2.utils import download, tokenizer\n",
    "# from kogpt2.model.torch_gpt2 import GPT2Config, GPT2LMHeadModel\n",
    "# from kogpt2.data import Read_Dataset\n",
    "# from kogpt2.model.sample import sample_sequence\n",
    "\n",
    "# https://github.com/haven-jeon/KoGPT2-chatbot/blob/master/train_torch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332235c1-180b-4328-b0ae-88e8ef5f9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3624dbe3-1fd1-443f-b2a1-99581b24392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(pretrained_model_name_or_path=\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>', sep_token=\"<sep>\", cls_token=\"<cls>\", additional_special_tokens=[\"test\", \"<turn>\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c099caf-0ebd-4a92-906c-88f656083a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '근육이 커진다 그렇다면 얼마나 커질까?'\n",
    "input_ids = tokenizer.encode(text)\n",
    "\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b91e2411-ee17-4d4e-8309-e38382b0ab63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33245, 10114, 8265, 7182, 24042, 15570, 45282, 47804]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d763c0e-ebb1-4536-b19f-4f0698effbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[33245, 10114,  8265,  7182, 24042, 15570, 45282, 47804, 16563,   377,\n",
       "          6947,  7406, 10846,  9022,  6855, 46651,  8234,   406,  9162, 12928,\n",
       "          9782,  9018, 18814,  9148, 47041, 14226, 11649,  9659,  7991,  9337,\n",
       "          6969,  8084,   376,  9394,  9863, 23066,  7283,  9745,  9736, 11064,\n",
       "          9080,  9548,  9290,  9054, 14675, 20192, 49421,  9198,  6958, 11357,\n",
       "         12201,   387, 11132, 15605, 15842, 23475,  9025, 13972,  9318,  7172,\n",
       "         14364,  9037,  7601,   389, 10351, 12857, 11793, 17582, 13023,  9277,\n",
       "         10578,  9432,  9098,  7071,  9348,  9078,  7801, 25856, 27747,  9094,\n",
       "         19802, 12687,  9267, 19933,  8267, 23498, 12521, 11403, 12102,  9427,\n",
       "          9036,  9515, 10917,  9135,  8718, 16691,  9194,  7194, 11991, 24692,\n",
       "          9207, 22375,  9676, 20289, 12487, 13358,  8263,  9111, 16650, 11777,\n",
       "         10021, 19520, 17932,  9320,  8052, 14662, 50082,  8146, 10431, 19935,\n",
       "         13875,  9258, 13701, 37993, 10138, 10899, 43056,  7162]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e66ae40c-b45c-49af-b27f-30f8c975bfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['근육이 커진다 그렇다면 얼마나 커질까?\"\\n\"그렇다면 그건 뭐죠? 당신은 지금 이 순간부터 당신의 몸을 어떻게 만들어야 할까요! 그리고 어떤 식으로든 자신의 몸 안에 있는 모든 것을 다 없애야만 합니다. 그러기 위해서는 우선, 자신이 원하는 대로 움직일 수 있어야 합니다, 제발. 하지만 이제부터는 내가 해야 하는 일을 모두 해내도록 하세요. 그러면 내 몸은 점점 더 작아질 겁니다. 그래서 나는 다시 한 번 이렇게 말했습니다. \\'당신이 나를 위해 무엇을 하고 있는지 알고 싶지 않다면, 그것은 바로 나의 몸이 아니오\\'라고.\"\\n이렇게 말하고 나서 그는 잠시 생각에 잠겼다.\\n그리고는']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(gen_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c81fd4c7-b4be-4fc8-835f-df4c303b001e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\\n특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\\n또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\\n아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\\n운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\\n운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\\n운동을']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(gen_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d18c8cb1-546d-43fb-91b3-e433f82f12d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 33245, 10114, 12748, 11357, 739, 3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73980dcd-1faa-46cf-b659-19f9b86064ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s> 근육이 커지기 위해서는 <pad>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/haven-jeon/KoGPT2-chatbot/blob/master/train_torch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, chats, max_len=32):\n",
    "        self._data = chats\n",
    "        self.first = True\n",
    "        self.q_token = U_TKN\n",
    "        self.a_token = S_TKN\n",
    "        self.sent_token = SENT\n",
    "        self.bos = BOS\n",
    "        self.eos = EOS\n",
    "        self.mask = MASK\n",
    "        self.pad = PAD\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = TOKENIZER \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        turn = self._data.iloc[idx]\n",
    "        q = turn['Q']\n",
    "        a = turn['A']\n",
    "        sentiment = str(turn['label'])\n",
    "        q_toked = self.tokenizer.tokenize(self.q_token + q + \\\n",
    "                                          self.sent_token + sentiment)   \n",
    "        q_len = len(q_toked)\n",
    "        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n",
    "        a_len = len(a_toked)\n",
    "        if q_len + a_len > self.max_len:\n",
    "            a_len = self.max_len - q_len\n",
    "            if a_len <= 0:\n",
    "                q_toked = q_toked[-(int(self.max_len/2)):]\n",
    "                q_len = len(q_toked)\n",
    "                a_len = self.max_len - q_len\n",
    "                assert a_len > 0\n",
    "            a_toked = a_toked[:a_len]\n",
    "            a_len = len(a_toked)\n",
    "            assert a_len == len(a_toked), f'{a_len} ==? {len(a_toked)}'\n",
    "        # [mask, mask, ...., mask, ..., <bos>,..A.. <eos>, <pad>....]\n",
    "        labels = [\n",
    "            self.mask,\n",
    "        ] * q_len + a_toked[1:]\n",
    "        if self.first:\n",
    "            logging.info(\"contexts : {}\".format(q))\n",
    "            logging.info(\"toked ctx: {}\".format(q_toked))\n",
    "            logging.info(\"response : {}\".format(a))\n",
    "            logging.info(\"toked response : {}\".format(a_toked))\n",
    "            logging.info('labels {}'.format(labels))\n",
    "            self.first = False\n",
    "        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n",
    "        self.max_len\n",
    "        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
    "        while len(labels_ids) < self.max_len:\n",
    "            labels_ids += [self.tokenizer.pad_token_id]\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n",
    "        while len(token_ids) < self.max_len:\n",
    "            token_ids += [self.tokenizer.pad_token_id]\n",
    "        return(token_ids, np.array(mask),\n",
    "               labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af9055b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T02:14:40.103687Z",
     "start_time": "2021-08-31T02:14:31.035455Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>') \n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ab344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epoch', type=int, default=200,\n",
    "\t\t\t\t\thelp=\"epoch 를 통해서 학습 범위를 조절합니다.\")\n",
    "parser.add_argument('--save_path', type=str, default='./checkpoint/',\n",
    "\t\t\t\t\thelp=\"학습 결과를 저장하는 경로입니다.\")\n",
    "parser.add_argument('--load_path', type=str, default='./checkpoint/Alls/KoGPT2_checkpoint_296000.tar', #\n",
    "\t\t\t\t\thelp=\"학습된 결과를 불러오는 경로입니다.\")\n",
    "parser.add_argument('--samples', type=str, default=\"samples/\",\n",
    "\t\t\t\t\thelp=\"생성 결과를 저장할 경로입니다.\")\n",
    "parser.add_argument('--data_file_path', type=str, default='dataset/lyrics_dataset.txt',\n",
    "\t\t\t\t\thelp=\"학습할 데이터를 불러오는 경로입니다.\")\n",
    "parser.add_argument('--batch_size', type=int, default=8,\n",
    "\t\t\t\t\thelp=\"batch_size 를 지정합니다.\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "pytorch_kogpt2 = {\n",
    "    'url':\n",
    "    'checkpoint/pytorch_kogpt2_676e9bcfa7.params',\n",
    "    'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
    "    'chksum': '676e9bcfa7'\n",
    "}\n",
    "\n",
    "kogpt2_config = {\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"layer_norm_epsilon\": 1e-05,\n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_head\": 12,\n",
    "    \"n_layer\": 12,\n",
    "    \"n_positions\": 1024,\n",
    "    \"vocab_size\": 50000\n",
    "}\n",
    "\n",
    "def auto_enter(text):\n",
    "\ttext = (text.replace(\"   \", \"\\n\"))\n",
    "\ttext = text.split(\"\\n\")\n",
    "\n",
    "\ttext = [t.lstrip() for t in text if t != '']\n",
    "\treturn \"\\n\\n\".join(text)\n",
    "\n",
    "def get_gpu_memory_map():\n",
    "\t\"\"\"Get the current gpu usage.\n",
    "\tReturns\n",
    "\t-------\n",
    "\tusage: dict\n",
    "\t\tKeys are device ids as integers.\n",
    "\t\tValues are memory usage as integers in MB.\n",
    "\t\"\"\"\n",
    "\tresult = subprocess.check_output(\n",
    "\t\t[\n",
    "\t\t\t'nvidia-smi', '--query-gpu=memory.used',\n",
    "\t\t\t'--format=csv,nounits,noheader'\n",
    "\t\t], encoding='utf-8')\n",
    "\t# Convert lines into a dictionary\n",
    "\tgpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
    "\tgpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
    "\treturn gpu_memory_map\n",
    "\n",
    "def main(epoch, save_path, load_path, samples, data_file_path, batch_size):\n",
    "\tctx = 'cuda'\n",
    "\tcachedir = '~/kogpt2/'\n",
    "\n",
    "\tsummary = SummaryWriter()\n",
    "\n",
    "\t# download model\n",
    "\tmodel_info = pytorch_kogpt2\n",
    "\tmodel_path = download(model_info['url'],\n",
    "\t\t\t\t\t\t   model_info['fname'],\n",
    "\t\t\t\t\t\t   model_info['chksum'],\n",
    "\t\t\t\t\t\t   cachedir=cachedir)\n",
    "\t# download vocab\n",
    "\tvocab_info = tokenizer\n",
    "\tvocab_path = download(vocab_info['url'],\n",
    "\t\t\t\t\t\t   vocab_info['fname'],\n",
    "\t\t\t\t\t\t   vocab_info['chksum'],\n",
    "\t\t\t\t\t\t   cachedir=cachedir)\n",
    "\n",
    "\t# KoGPT-2 언어 모델 학습을 위한 GPT2LMHeadModel 선언\n",
    "\tkogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
    "\n",
    "\t# model_path 로부터 다운로드 받은 내용을 load_state_dict 으로 업로드\n",
    "\tkogpt2model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\tdevice = torch.device(ctx)\n",
    "\tkogpt2model.to(device)\n",
    "\n",
    "\t# 불러오기 부분\n",
    "\ttry:\n",
    "\t\tcheckpoint = torch.load(load_path, map_location=device)\n",
    "\n",
    "\t\t# KoGPT-2 언어 모델 학습을 위한 GPT2LMHeadModel 선언\n",
    "\t\tkogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
    "\t\tkogpt2model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\t\tkogpt2model.eval()\n",
    "\texcept:\n",
    "\t\tcount = 0\n",
    "\telse:\n",
    "\t\tcount = int(re.findall(\"\\d+\", load_path)[1])\n",
    "\n",
    "\tprint(count)\n",
    "\t# 추가로 학습하기 위해 .train() 사용\n",
    "\tkogpt2model.train()\n",
    "\tvocab_b_obj = gluonnlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t mask_token=None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sep_token=None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t cls_token=None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t unknown_token='<unk>',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t padding_token='<pad>',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t bos_token='<s>',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t eos_token='</s>')\n",
    "\n",
    "\n",
    "\ttok_path = get_tokenizer()\n",
    "\tmodel, vocab = kogpt2model, vocab_b_obj\n",
    "\ttok = SentencepieceTokenizer(tok_path)\n",
    "\n",
    "\tdataset = Read_Dataset(data_file_path, vocab, tok)\n",
    "\tprint(\"Read_Dataset ok\")\n",
    "\tdata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "\tlearning_rate = 3e-5\n",
    "\tcriterion = torch.nn.CrossEntropyLoss()\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\tprint('KoGPT-2 Transfer Learning Start')\n",
    "\tavg_loss = (0.0, 0.0)\n",
    "\n",
    "\tfor epoch in range(epoch):\n",
    "\t\tfor data in data_loader:\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tdata = torch.stack(data) # list of Tensor로 구성되어 있기 때문에 list를 stack을 통해 변환해준다.\n",
    "\t\t\tdata = data.transpose(1,0)\n",
    "\t\t\tdata = data.to(ctx)\n",
    "\t\t\tmodel = model.to(ctx)\n",
    "\n",
    "\t\t\toutputs = model(data, labels=data)\n",
    "\t\t\tloss, logits = outputs[:2]\n",
    "\t\t\tloss = loss.to(ctx)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\tavg_loss = (avg_loss[0] * 0.99 + loss, avg_loss[1] * 0.99 + 1.0)\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tif count % 10 == 0:\n",
    "\t\t\t\tprint('epoch no.{0} train no.{1}  loss = {2:.5f} avg_loss = {3:.5f}' . format(epoch, count, loss, avg_loss[0] / avg_loss[1]))\n",
    "\t\t\t\tsummary.add_scalar('loss/avg_loss', avg_loss[0] / avg_loss[1], count)\n",
    "\t\t\t\tsummary.add_scalar('loss/loss', loss, count)\n",
    "\n",
    "\t\t\t# generator 진행\n",
    "\t\t\tif (count > 0 and count % 1000 == 0) or (len(data) < batch_size):\n",
    "\t\t\t\tsent = sample_sequence(model.to(\"cpu\"), tok, vocab, sent=\"사랑\", text_size=100, temperature=0.7, top_p=0.8, top_k=40)\n",
    "\t\t\t\tsent = sent.replace(\"<unused0>\", \"\\n\") # 비효율적이지만 엔터를 위해서 등장\n",
    "\t\t\t\tsent = auto_enter(sent)\n",
    "\t\t\t\tprint(sent)\n",
    "\n",
    "\t\t\t\tsummary.add_text('Text', sent, count)\n",
    "\n",
    "\t\t\t\tif count > 500000:\n",
    "\t\t\t\t\tnow = [int(n) for n in os.listdir(samples)]\n",
    "\t\t\t\t\tnow = max(now)\n",
    "\t\t\t\t\tf = open(samples + str(now + 1), 'w', encoding=\"utf-8\")\n",
    "\t\t\t\t\tf.write(sent)\n",
    "\t\t\t\t\tf.close()\n",
    "\t\t\t#########################################\n",
    "\t\t\tcount += 1\n",
    "\n",
    "\t\t\tif (count > 0 and count % 10000 == 0) or (len(data) < batch_size):\n",
    "\t\t\t\t# 모델 저장\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\ttorch.save({\n",
    "\t\t\t\t\t\t'epoch': epoch,\n",
    "\t\t\t\t\t\t'train_no': count,\n",
    "\t\t\t\t\t\t'model_state_dict': model.state_dict(),\n",
    "\t\t\t\t\t\t'optimizer_state_dict': optimizer.state_dict(),\n",
    "\t\t\t\t\t\t'loss': loss\n",
    "\t\t\t\t\t}, save_path + 'KoGPT2_checkpoint_' + str(count) + '.tar')\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tpass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain(args.epoch, args.save_path, args.load_path, args.samples, args.data_file_path, args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a413a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a306cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03880e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b2c85b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T02:20:58.427772Z",
     "start_time": "2021-08-31T02:20:58.411774Z"
    }
   },
   "outputs": [],
   "source": [
    "text = '근육이 커지기 위해서는'\n",
    "input_ids = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17f17e74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T02:21:12.195055Z",
     "start_time": "2021-08-31T02:21:12.178084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33245, 10114, 12748, 11357]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fa500dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T01:52:08.814546Z",
     "start_time": "2021-08-31T01:52:08.796991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\\n특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\\n또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\\n아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\\n운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\\n운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\\n운동을'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "110f31f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T01:52:04.344789Z",
     "start_time": "2021-08-31T01:51:59.590307Z"
    }
   },
   "outputs": [],
   "source": [
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
