{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b66a8b7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T02:14:30.135574Z",
     "start_time": "2021-08-31T02:14:26.942912Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import subprocess\n",
    "# import gluonnlp\n",
    "import torch\n",
    "from torch.utils.data import DataLoader \n",
    "from gluonnlp.data import SentencepieceTokenizer \n",
    "\n",
    "# tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\")\n",
    "# https://hipgyung.tistory.com/entry/%ED%95%9C%EA%B5%AD%EC%96%B4-%EA%B0%80%EC%82%AC-%EB%8D%B0%EC%9D%B4%ED%84%B0-KoGPT2-Fine-Tuning\n",
    "# https://github.com/gyunggyung/KoGPT2-FineTuning\n",
    "# https://ratsgo.github.io/nlpbook/docs/generation/inference2/\n",
    "\n",
    "# git clone https://github.com/SKT-AI/KoGPT2.git\n",
    "# cd KoGPT2\n",
    "# pip install -r requirements.txt\n",
    "# pip install .\n",
    "# from kogpt2.utils import get_tokenizer\n",
    "# from kogpt2.utils import download, tokenizer\n",
    "# from kogpt2.model.torch_gpt2 import GPT2Config, GPT2LMHeadModel\n",
    "# from kogpt2.data import Read_Dataset\n",
    "# from kogpt2.model.sample import sample_sequence\n",
    "\n",
    "# https://github.com/haven-jeon/KoGPT2-chatbot/blob/master/train_torch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332235c1-180b-4328-b0ae-88e8ef5f9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3624dbe3-1fd1-443f-b2a1-99581b24392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(pretrained_model_name_or_path=\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>', sep_token=\"<sep>\", cls_token=\"<cls>\", additional_special_tokens=[\"test\", \"<turn>\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c099caf-0ebd-4a92-906c-88f656083a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'ê·¼ìœ¡ì´ ì»¤ì§„ë‹¤ ê·¸ë ‡ë‹¤ë©´ ì–¼ë§ˆë‚˜ ì»¤ì§ˆê¹Œ?'\n",
    "input_ids = tokenizer.encode(text)\n",
    "\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b91e2411-ee17-4d4e-8309-e38382b0ab63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33245, 10114, 8265, 7182, 24042, 15570, 45282, 47804]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d763c0e-ebb1-4536-b19f-4f0698effbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[33245, 10114,  8265,  7182, 24042, 15570, 45282, 47804, 16563,   377,\n",
       "          6947,  7406, 10846,  9022,  6855, 46651,  8234,   406,  9162, 12928,\n",
       "          9782,  9018, 18814,  9148, 47041, 14226, 11649,  9659,  7991,  9337,\n",
       "          6969,  8084,   376,  9394,  9863, 23066,  7283,  9745,  9736, 11064,\n",
       "          9080,  9548,  9290,  9054, 14675, 20192, 49421,  9198,  6958, 11357,\n",
       "         12201,   387, 11132, 15605, 15842, 23475,  9025, 13972,  9318,  7172,\n",
       "         14364,  9037,  7601,   389, 10351, 12857, 11793, 17582, 13023,  9277,\n",
       "         10578,  9432,  9098,  7071,  9348,  9078,  7801, 25856, 27747,  9094,\n",
       "         19802, 12687,  9267, 19933,  8267, 23498, 12521, 11403, 12102,  9427,\n",
       "          9036,  9515, 10917,  9135,  8718, 16691,  9194,  7194, 11991, 24692,\n",
       "          9207, 22375,  9676, 20289, 12487, 13358,  8263,  9111, 16650, 11777,\n",
       "         10021, 19520, 17932,  9320,  8052, 14662, 50082,  8146, 10431, 19935,\n",
       "         13875,  9258, 13701, 37993, 10138, 10899, 43056,  7162]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e66ae40c-b45c-49af-b27f-30f8c975bfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ê·¼ìœ¡ì´ ì»¤ì§„ë‹¤ ê·¸ë ‡ë‹¤ë©´ ì–¼ë§ˆë‚˜ ì»¤ì§ˆê¹Œ?\"\\n\"ê·¸ë ‡ë‹¤ë©´ ê·¸ê±´ ë­ì£ ? ë‹¹ì‹ ì€ ì§€ê¸ˆ ì´ ìˆœê°„ë¶€í„° ë‹¹ì‹ ì˜ ëª¸ì„ ì–´ë–»ê²Œ ë§Œë“¤ì–´ì•¼ í• ê¹Œìš”! ê·¸ë¦¬ê³  ì–´ë–¤ ì‹ìœ¼ë¡œë“  ìì‹ ì˜ ëª¸ ì•ˆì— ìˆëŠ” ëª¨ë“  ê²ƒì„ ë‹¤ ì—†ì• ì•¼ë§Œ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” ìš°ì„ , ìì‹ ì´ ì›í•˜ëŠ” ëŒ€ë¡œ ì›€ì§ì¼ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤, ì œë°œ. í•˜ì§€ë§Œ ì´ì œë¶€í„°ëŠ” ë‚´ê°€ í•´ì•¼ í•˜ëŠ” ì¼ì„ ëª¨ë‘ í•´ë‚´ë„ë¡ í•˜ì„¸ìš”. ê·¸ëŸ¬ë©´ ë‚´ ëª¸ì€ ì ì  ë” ì‘ì•„ì§ˆ ê²ë‹ˆë‹¤. ê·¸ë˜ì„œ ë‚˜ëŠ” ë‹¤ì‹œ í•œ ë²ˆ ì´ë ‡ê²Œ ë§í–ˆìŠµë‹ˆë‹¤. \\'ë‹¹ì‹ ì´ ë‚˜ë¥¼ ìœ„í•´ ë¬´ì—‡ì„ í•˜ê³  ìˆëŠ”ì§€ ì•Œê³  ì‹¶ì§€ ì•Šë‹¤ë©´, ê·¸ê²ƒì€ ë°”ë¡œ ë‚˜ì˜ ëª¸ì´ ì•„ë‹ˆì˜¤\\'ë¼ê³ .\"\\nì´ë ‡ê²Œ ë§í•˜ê³  ë‚˜ì„œ ê·¸ëŠ” ì ì‹œ ìƒê°ì— ì ê²¼ë‹¤.\\nê·¸ë¦¬ê³ ëŠ”']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(gen_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c81fd4c7-b4be-4fc8-835f-df4c303b001e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ë¬´ì—‡ë³´ë‹¤ ê·œì¹™ì ì¸ ìƒí™œìŠµê´€ì´ ì¤‘ìš”í•˜ë‹¤.\\níŠ¹íˆ, ì•„ì¹¨ì‹ì‚¬ëŠ” ë‹¨ë°±ì§ˆê³¼ ë¹„íƒ€ë¯¼ì´ í’ë¶€í•œ ê³¼ì¼ê³¼ ì±„ì†Œë¥¼ ë§ì´ ì„­ì·¨í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\\në˜í•œ í•˜ë£¨ 30ë¶„ ì´ìƒ ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ëŠ” ê²ƒë„ ë„ì›€ì´ ëœë‹¤.\\nì•„ì¹¨ ì‹ì‚¬ë¥¼ ê±°ë¥´ì§€ ì•Šê³  ê·œì¹™ì ìœ¼ë¡œ ìš´ë™ì„ í•˜ë©´ í˜ˆì•¡ìˆœí™˜ì— ë„ì›€ì„ ì¤„ ë¿ë§Œ ì•„ë‹ˆë¼ ì‹ ì§„ëŒ€ì‚¬ë¥¼ ì´‰ì§„í•´ ì²´ë‚´ ë…¸íë¬¼ì„ ë°°ì¶œí•˜ê³  í˜ˆì••ì„ ë‚®ì¶°ì¤€ë‹¤.\\nìš´ë™ì€ í•˜ë£¨ì— 10ë¶„ ì •ë„ë§Œ í•˜ëŠ” ê²Œ ì¢‹ìœ¼ë©° ìš´ë™ í›„ì—ëŠ” ë°˜ë“œì‹œ ìŠ¤íŠ¸ë ˆì¹­ì„ í†µí•´ ê·¼ìœ¡ëŸ‰ì„ ëŠ˜ë¦¬ê³  ìœ ì—°ì„±ì„ ë†’ì—¬ì•¼ í•œë‹¤.\\nìš´ë™ í›„ ë°”ë¡œ ì ìë¦¬ì— ë“œëŠ” ê²ƒì€ í”¼í•´ì•¼ í•˜ë©° íŠ¹íˆ ì•„ì¹¨ì— ì¼ì–´ë‚˜ë©´ ëª¸ì´ í”¼ê³¤í•´ì§€ê¸° ë•Œë¬¸ì— ë¬´ë¦¬í•˜ê²Œ ì›€ì§ì´ë©´ ì˜¤íˆë ¤ ì—­íš¨ê³¼ê°€ ë‚  ìˆ˜ë„ ìˆë‹¤.\\nìš´ë™ì„']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(gen_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d18c8cb1-546d-43fb-91b3-e433f82f12d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 33245, 10114, 12748, 11357, 739, 3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73980dcd-1faa-46cf-b659-19f9b86064ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s> ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” <pad>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/haven-jeon/KoGPT2-chatbot/blob/master/train_torch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, chats, max_len=32):\n",
    "        self._data = chats\n",
    "        self.first = True\n",
    "        self.q_token = U_TKN\n",
    "        self.a_token = S_TKN\n",
    "        self.sent_token = SENT\n",
    "        self.bos = BOS\n",
    "        self.eos = EOS\n",
    "        self.mask = MASK\n",
    "        self.pad = PAD\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = TOKENIZER \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        turn = self._data.iloc[idx]\n",
    "        q = turn['Q']\n",
    "        a = turn['A']\n",
    "        sentiment = str(turn['label'])\n",
    "        q_toked = self.tokenizer.tokenize(self.q_token + q + \\\n",
    "                                          self.sent_token + sentiment)   \n",
    "        q_len = len(q_toked)\n",
    "        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n",
    "        a_len = len(a_toked)\n",
    "        if q_len + a_len > self.max_len:\n",
    "            a_len = self.max_len - q_len\n",
    "            if a_len <= 0:\n",
    "                q_toked = q_toked[-(int(self.max_len/2)):]\n",
    "                q_len = len(q_toked)\n",
    "                a_len = self.max_len - q_len\n",
    "                assert a_len > 0\n",
    "            a_toked = a_toked[:a_len]\n",
    "            a_len = len(a_toked)\n",
    "            assert a_len == len(a_toked), f'{a_len} ==? {len(a_toked)}'\n",
    "        # [mask, mask, ...., mask, ..., <bos>,..A.. <eos>, <pad>....]\n",
    "        labels = [\n",
    "            self.mask,\n",
    "        ] * q_len + a_toked[1:]\n",
    "        if self.first:\n",
    "            logging.info(\"contexts : {}\".format(q))\n",
    "            logging.info(\"toked ctx: {}\".format(q_toked))\n",
    "            logging.info(\"response : {}\".format(a))\n",
    "            logging.info(\"toked response : {}\".format(a_toked))\n",
    "            logging.info('labels {}'.format(labels))\n",
    "            self.first = False\n",
    "        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n",
    "        self.max_len\n",
    "        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
    "        while len(labels_ids) < self.max_len:\n",
    "            labels_ids += [self.tokenizer.pad_token_id]\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n",
    "        while len(token_ids) < self.max_len:\n",
    "            token_ids += [self.tokenizer.pad_token_id]\n",
    "        return(token_ids, np.array(mask),\n",
    "               labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af9055b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T02:14:40.103687Z",
     "start_time": "2021-08-31T02:14:31.035455Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>') \n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ab344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epoch', type=int, default=200,\n",
    "\t\t\t\t\thelp=\"epoch ë¥¼ í†µí•´ì„œ í•™ìŠµ ë²”ìœ„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.\")\n",
    "parser.add_argument('--save_path', type=str, default='./checkpoint/',\n",
    "\t\t\t\t\thelp=\"í•™ìŠµ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ê²½ë¡œì…ë‹ˆë‹¤.\")\n",
    "parser.add_argument('--load_path', type=str, default='./checkpoint/Alls/KoGPT2_checkpoint_296000.tar', #\n",
    "\t\t\t\t\thelp=\"í•™ìŠµëœ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê²½ë¡œì…ë‹ˆë‹¤.\")\n",
    "parser.add_argument('--samples', type=str, default=\"samples/\",\n",
    "\t\t\t\t\thelp=\"ìƒì„± ê²°ê³¼ë¥¼ ì €ì¥í•  ê²½ë¡œì…ë‹ˆë‹¤.\")\n",
    "parser.add_argument('--data_file_path', type=str, default='dataset/lyrics_dataset.txt',\n",
    "\t\t\t\t\thelp=\"í•™ìŠµí•  ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê²½ë¡œì…ë‹ˆë‹¤.\")\n",
    "parser.add_argument('--batch_size', type=int, default=8,\n",
    "\t\t\t\t\thelp=\"batch_size ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "pytorch_kogpt2 = {\n",
    "    'url':\n",
    "    'checkpoint/pytorch_kogpt2_676e9bcfa7.params',\n",
    "    'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
    "    'chksum': '676e9bcfa7'\n",
    "}\n",
    "\n",
    "kogpt2_config = {\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"layer_norm_epsilon\": 1e-05,\n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_head\": 12,\n",
    "    \"n_layer\": 12,\n",
    "    \"n_positions\": 1024,\n",
    "    \"vocab_size\": 50000\n",
    "}\n",
    "\n",
    "def auto_enter(text):\n",
    "\ttext = (text.replace(\"   \", \"\\n\"))\n",
    "\ttext = text.split(\"\\n\")\n",
    "\n",
    "\ttext = [t.lstrip() for t in text if t != '']\n",
    "\treturn \"\\n\\n\".join(text)\n",
    "\n",
    "def get_gpu_memory_map():\n",
    "\t\"\"\"Get the current gpu usage.\n",
    "\tReturns\n",
    "\t-------\n",
    "\tusage: dict\n",
    "\t\tKeys are device ids as integers.\n",
    "\t\tValues are memory usage as integers in MB.\n",
    "\t\"\"\"\n",
    "\tresult = subprocess.check_output(\n",
    "\t\t[\n",
    "\t\t\t'nvidia-smi', '--query-gpu=memory.used',\n",
    "\t\t\t'--format=csv,nounits,noheader'\n",
    "\t\t], encoding='utf-8')\n",
    "\t# Convert lines into a dictionary\n",
    "\tgpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
    "\tgpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
    "\treturn gpu_memory_map\n",
    "\n",
    "def main(epoch, save_path, load_path, samples, data_file_path, batch_size):\n",
    "\tctx = 'cuda'\n",
    "\tcachedir = '~/kogpt2/'\n",
    "\n",
    "\tsummary = SummaryWriter()\n",
    "\n",
    "\t# download model\n",
    "\tmodel_info = pytorch_kogpt2\n",
    "\tmodel_path = download(model_info['url'],\n",
    "\t\t\t\t\t\t   model_info['fname'],\n",
    "\t\t\t\t\t\t   model_info['chksum'],\n",
    "\t\t\t\t\t\t   cachedir=cachedir)\n",
    "\t# download vocab\n",
    "\tvocab_info = tokenizer\n",
    "\tvocab_path = download(vocab_info['url'],\n",
    "\t\t\t\t\t\t   vocab_info['fname'],\n",
    "\t\t\t\t\t\t   vocab_info['chksum'],\n",
    "\t\t\t\t\t\t   cachedir=cachedir)\n",
    "\n",
    "\t# KoGPT-2 ì–¸ì–´ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ GPT2LMHeadModel ì„ ì–¸\n",
    "\tkogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
    "\n",
    "\t# model_path ë¡œë¶€í„° ë‹¤ìš´ë¡œë“œ ë°›ì€ ë‚´ìš©ì„ load_state_dict ìœ¼ë¡œ ì—…ë¡œë“œ\n",
    "\tkogpt2model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\tdevice = torch.device(ctx)\n",
    "\tkogpt2model.to(device)\n",
    "\n",
    "\t# ë¶ˆëŸ¬ì˜¤ê¸° ë¶€ë¶„\n",
    "\ttry:\n",
    "\t\tcheckpoint = torch.load(load_path, map_location=device)\n",
    "\n",
    "\t\t# KoGPT-2 ì–¸ì–´ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ GPT2LMHeadModel ì„ ì–¸\n",
    "\t\tkogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
    "\t\tkogpt2model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\t\tkogpt2model.eval()\n",
    "\texcept:\n",
    "\t\tcount = 0\n",
    "\telse:\n",
    "\t\tcount = int(re.findall(\"\\d+\", load_path)[1])\n",
    "\n",
    "\tprint(count)\n",
    "\t# ì¶”ê°€ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•´ .train() ì‚¬ìš©\n",
    "\tkogpt2model.train()\n",
    "\tvocab_b_obj = gluonnlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t mask_token=None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sep_token=None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t cls_token=None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t unknown_token='<unk>',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t padding_token='<pad>',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t bos_token='<s>',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t eos_token='</s>')\n",
    "\n",
    "\n",
    "\ttok_path = get_tokenizer()\n",
    "\tmodel, vocab = kogpt2model, vocab_b_obj\n",
    "\ttok = SentencepieceTokenizer(tok_path)\n",
    "\n",
    "\tdataset = Read_Dataset(data_file_path, vocab, tok)\n",
    "\tprint(\"Read_Dataset ok\")\n",
    "\tdata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "\tlearning_rate = 3e-5\n",
    "\tcriterion = torch.nn.CrossEntropyLoss()\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\tprint('KoGPT-2 Transfer Learning Start')\n",
    "\tavg_loss = (0.0, 0.0)\n",
    "\n",
    "\tfor epoch in range(epoch):\n",
    "\t\tfor data in data_loader:\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tdata = torch.stack(data) # list of Tensorë¡œ êµ¬ì„±ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— listë¥¼ stackì„ í†µí•´ ë³€í™˜í•´ì¤€ë‹¤.\n",
    "\t\t\tdata = data.transpose(1,0)\n",
    "\t\t\tdata = data.to(ctx)\n",
    "\t\t\tmodel = model.to(ctx)\n",
    "\n",
    "\t\t\toutputs = model(data, labels=data)\n",
    "\t\t\tloss, logits = outputs[:2]\n",
    "\t\t\tloss = loss.to(ctx)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\tavg_loss = (avg_loss[0] * 0.99 + loss, avg_loss[1] * 0.99 + 1.0)\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tif count % 10 == 0:\n",
    "\t\t\t\tprint('epoch no.{0} train no.{1}  loss = {2:.5f} avg_loss = {3:.5f}' . format(epoch, count, loss, avg_loss[0] / avg_loss[1]))\n",
    "\t\t\t\tsummary.add_scalar('loss/avg_loss', avg_loss[0] / avg_loss[1], count)\n",
    "\t\t\t\tsummary.add_scalar('loss/loss', loss, count)\n",
    "\n",
    "\t\t\t# generator ì§„í–‰\n",
    "\t\t\tif (count > 0 and count % 1000 == 0) or (len(data) < batch_size):\n",
    "\t\t\t\tsent = sample_sequence(model.to(\"cpu\"), tok, vocab, sent=\"ì‚¬ë‘\", text_size=100, temperature=0.7, top_p=0.8, top_k=40)\n",
    "\t\t\t\tsent = sent.replace(\"<unused0>\", \"\\n\") # ë¹„íš¨ìœ¨ì ì´ì§€ë§Œ ì—”í„°ë¥¼ ìœ„í•´ì„œ ë“±ì¥\n",
    "\t\t\t\tsent = auto_enter(sent)\n",
    "\t\t\t\tprint(sent)\n",
    "\n",
    "\t\t\t\tsummary.add_text('Text', sent, count)\n",
    "\n",
    "\t\t\t\tif count > 500000:\n",
    "\t\t\t\t\tnow = [int(n) for n in os.listdir(samples)]\n",
    "\t\t\t\t\tnow = max(now)\n",
    "\t\t\t\t\tf = open(samples + str(now + 1), 'w', encoding=\"utf-8\")\n",
    "\t\t\t\t\tf.write(sent)\n",
    "\t\t\t\t\tf.close()\n",
    "\t\t\t#########################################\n",
    "\t\t\tcount += 1\n",
    "\n",
    "\t\t\tif (count > 0 and count % 10000 == 0) or (len(data) < batch_size):\n",
    "\t\t\t\t# ëª¨ë¸ ì €ì¥\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\ttorch.save({\n",
    "\t\t\t\t\t\t'epoch': epoch,\n",
    "\t\t\t\t\t\t'train_no': count,\n",
    "\t\t\t\t\t\t'model_state_dict': model.state_dict(),\n",
    "\t\t\t\t\t\t'optimizer_state_dict': optimizer.state_dict(),\n",
    "\t\t\t\t\t\t'loss': loss\n",
    "\t\t\t\t\t}, save_path + 'KoGPT2_checkpoint_' + str(count) + '.tar')\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tpass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain(args.epoch, args.save_path, args.load_path, args.samples, args.data_file_path, args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a413a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a306cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03880e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b2c85b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T02:20:58.427772Z",
     "start_time": "2021-08-31T02:20:58.411774Z"
    }
   },
   "outputs": [],
   "source": [
    "text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'\n",
    "input_ids = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17f17e74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T02:21:12.195055Z",
     "start_time": "2021-08-31T02:21:12.178084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33245, 10114, 12748, 11357]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fa500dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T01:52:08.814546Z",
     "start_time": "2021-08-31T01:52:08.796991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ë¬´ì—‡ë³´ë‹¤ ê·œì¹™ì ì¸ ìƒí™œìŠµê´€ì´ ì¤‘ìš”í•˜ë‹¤.\\níŠ¹íˆ, ì•„ì¹¨ì‹ì‚¬ëŠ” ë‹¨ë°±ì§ˆê³¼ ë¹„íƒ€ë¯¼ì´ í’ë¶€í•œ ê³¼ì¼ê³¼ ì±„ì†Œë¥¼ ë§ì´ ì„­ì·¨í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\\në˜í•œ í•˜ë£¨ 30ë¶„ ì´ìƒ ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ëŠ” ê²ƒë„ ë„ì›€ì´ ëœë‹¤.\\nì•„ì¹¨ ì‹ì‚¬ë¥¼ ê±°ë¥´ì§€ ì•Šê³  ê·œì¹™ì ìœ¼ë¡œ ìš´ë™ì„ í•˜ë©´ í˜ˆì•¡ìˆœí™˜ì— ë„ì›€ì„ ì¤„ ë¿ë§Œ ì•„ë‹ˆë¼ ì‹ ì§„ëŒ€ì‚¬ë¥¼ ì´‰ì§„í•´ ì²´ë‚´ ë…¸íë¬¼ì„ ë°°ì¶œí•˜ê³  í˜ˆì••ì„ ë‚®ì¶°ì¤€ë‹¤.\\nìš´ë™ì€ í•˜ë£¨ì— 10ë¶„ ì •ë„ë§Œ í•˜ëŠ” ê²Œ ì¢‹ìœ¼ë©° ìš´ë™ í›„ì—ëŠ” ë°˜ë“œì‹œ ìŠ¤íŠ¸ë ˆì¹­ì„ í†µí•´ ê·¼ìœ¡ëŸ‰ì„ ëŠ˜ë¦¬ê³  ìœ ì—°ì„±ì„ ë†’ì—¬ì•¼ í•œë‹¤.\\nìš´ë™ í›„ ë°”ë¡œ ì ìë¦¬ì— ë“œëŠ” ê²ƒì€ í”¼í•´ì•¼ í•˜ë©° íŠ¹íˆ ì•„ì¹¨ì— ì¼ì–´ë‚˜ë©´ ëª¸ì´ í”¼ê³¤í•´ì§€ê¸° ë•Œë¬¸ì— ë¬´ë¦¬í•˜ê²Œ ì›€ì§ì´ë©´ ì˜¤íˆë ¤ ì—­íš¨ê³¼ê°€ ë‚  ìˆ˜ë„ ìˆë‹¤.\\nìš´ë™ì„'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "110f31f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T01:52:04.344789Z",
     "start_time": "2021-08-31T01:51:59.590307Z"
    }
   },
   "outputs": [],
   "source": [
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
