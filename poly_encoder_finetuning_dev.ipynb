{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T06:13:03.538709Z",
     "start_time": "2021-10-06T06:13:03.524712Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from setproctitle import setproctitle\n",
    "setproctitle(\"Hodong_PolyEncoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T06:13:07.492010Z",
     "start_time": "2021-10-06T06:13:03.540710Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformer.data.retriever_dataset import ElectraDatasetFromDir, RetrieverDataLoader\n",
    "from transformer.tokenizer.utils import make_custom_tokenizer_from_pretrained, load_tokenizer_from_pretrained\n",
    "from transformer.layers.embedding import EmbeddingAggregation\n",
    "from transformer.models.interface import TrainHistory\n",
    "from transformer.models.utils import load_state_dict, init_path, get_score_json\n",
    "from transformer.utils.information_retrieval import BM25Okapi\n",
    "from transformer.models.poly_encoder import PolyEncoder\n",
    "from transformer.utils.common import set_device, convert_to_tensor, convert_to_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set WorkingDirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T06:13:07.506895Z",
     "start_time": "2021-10-06T06:13:07.493894Z"
    }
   },
   "outputs": [],
   "source": [
    "# # AIBUD_DEV\n",
    "# dataset_dir = \"/Users/aibud_dev/_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# # Korea_Server\n",
    "# dataset_dir = \"/home/mnt/guest1\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# # bigshane_local\n",
    "# dataset_dir = \"D:\\_jupyter\"\n",
    "# path = \"./config/file_path.json\"\n",
    "# file_path = None\n",
    "# with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "#     file_path = json.load(fp)\n",
    "\n",
    "# AWS\n",
    "dataset_dir = \"/home/ubuntu/data\"\n",
    "path = \"./config/file_path.json\"\n",
    "file_path = None\n",
    "with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "    file_path = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T06:13:07.612089Z",
     "start_time": "2021-10-06T06:13:07.507895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained huggingface_tokenizer: 'D:\\_jupyter/huggingface_tokenizer/kor/koelectra-vanila'\n",
      "vocab_size: 32200\n"
     ]
    }
   ],
   "source": [
    "tokenizer_file_path = dataset_dir + \"/huggingface_tokenizer/kor/koelectra-vanila\"\n",
    "\n",
    "# # save tokenizer to local\n",
    "# tokenizer_path = \"monologg/koelectra-base-discriminator\"\n",
    "# add_special_token = True\n",
    "# tokenizer = make_custom_tokenizer_from_pretrained(model_type=\"electra\", name_or_path=tokenizer_path, add_special_token=add_special_token)\n",
    "# tokenizer.save_pretrained(tokenizer_file_path)\n",
    "\n",
    "tokenizer = load_tokenizer_from_pretrained(model_type=\"electra\", name_or_path=tokenizer_file_path)\n",
    "print(\"vocab_size:\", len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T06:13:14.129790Z",
     "start_time": "2021-10-06T06:13:07.612982Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing data: 100%|█████████████████████████████████████████████████████████| 3481/3481 [00:06<00:00, 548.21it/s]\n"
     ]
    }
   ],
   "source": [
    "timesteps = 128\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "nprocs = 1\n",
    "\n",
    "dataset_name = \"four_n2x8_both\"\n",
    "total_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/retriever/{}/\".format(dataset_name)\n",
    "sample_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/retriever/{}/sample/\".format(dataset_name)\n",
    "train_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/retriever/{}/train/\".format(dataset_name)\n",
    "val_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/retriever/{}/val/\".format(dataset_name)\n",
    "test_data_dir = dataset_dir + \"/dataset/preprocessed/dialog_finetuning/retriever/{}/test/\".format(dataset_name)\n",
    "\n",
    "# total_dataset = ElectraDatasetFromDir(data_dir=total_data_dir, tokenizer=tokenizer, timesteps=timesteps, batch_size=batch_size, device=device, nprocs=nprocs)\n",
    "\n",
    "# train_dataset = ElectraDatasetFromDir(data_dir=train_data_dir, tokenizer=tokenizer, timesteps=timesteps, batch_size=batch_size, device=device, nprocs=nprocs)\n",
    "# train_data_loader = RetrieverDataLoader(dataset=train_dataset, batch_size=batch_size, device=device)\n",
    "\n",
    "# val_dataset = ElectraDatasetFromDir(data_dir=val_data_dir, tokenizer=tokenizer, timesteps=timesteps, batch_size=batch_size, device=device, nprocs=nprocs)\n",
    "# val_data_loader = RetrieverDataLoader(dataset=val_dataset, batch_size=batch_size, device=device)\n",
    "\n",
    "test_dataset = ElectraDatasetFromDir(data_dir=test_data_dir, tokenizer=tokenizer, timesteps=timesteps, batch_size=batch_size, device=device, nprocs=nprocs)\n",
    "test_data_loader = RetrieverDataLoader(dataset=test_dataset, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T06:13:14.144820Z",
     "start_time": "2021-10-06T06:13:14.132383Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_data_loader.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T07:05:35.541539Z",
     "start_time": "2021-10-06T07:05:20.200430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'temp_dir' has been set to './20211006_160520/' to save model while training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bigshane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Extracting responses: 100%|█████████████████████████████████████████████████████| 3481/3481 [00:00<00:00, 62158.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3218 candidates has been extracted\n",
      "'temp_dir' has been set to './20211006_160525/' to save model while training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bigshane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# bm25\n",
    "bm25 = BM25Okapi(tokenizer=tokenizer, dataset=test_dataset)\n",
    "\n",
    "# poly_encoder\n",
    "poly_encoder = PolyEncoder(encoder_type=\"electra\", vocab_size=len(tokenizer), m_code=64)\n",
    "optimizer = poly_encoder.get_optimizer(lr=3e-5)\n",
    "\n",
    "poly_encoder = set_device(poly_encoder, device=device)\n",
    "optimizer = set_device(optimizer, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30\n",
    "model_dir = dataset_dir + \"/model/poly_encoder/v3/{}/\".format(dataset_name)\n",
    "init_path(model_dir, True)\n",
    "\n",
    "metrics = [\"hits\", \"semantic_score\"]\n",
    "hits_k = [1, 2, 5, 10]\n",
    "name_or_path = \"beomi/kcbert-base\"\n",
    "\n",
    "train_history = TrainHistory()\n",
    "val_history = TrainHistory()\n",
    "for _epoch in range(1, epoch+1):\n",
    "    # train\n",
    "    epoch_train_history = poly_encoder.iteration_epoch(data_loader=train_data_loader, optimizer=optimizer, device=device, train=True, verbose_per_batch=-1)\n",
    "    # compute_scoresvn\n",
    "    train_candidates, train_candidate_embeds = poly_encoder.extract_candidates(device=device, dataset=train_dataset, additional_responses=None, concat_candidates=True, verbose=False)\n",
    "    train_scores = poly_encoder.compute_scores(metrics=metrics, tokenizer=tokenizer, data_loader=train_data_loader, device=device, hits_k=hits_k, name_or_path=name_or_path, candidates=train_candidates, candidate_embeds=train_candidate_embeds)\n",
    "    for metric, metric_score in train_scores.items():\n",
    "        epoch_train_history._add_acc(name=metric, value=metric_score)\n",
    "        \n",
    "    epoch_train_history_str = poly_encoder.verbose_template.format(mode=\"Epoch_train\", device=device, idx=_epoch, num_iters=epoch) + str(epoch_train_history)\n",
    "    print(epoch_train_history_str)\n",
    "    train_history += epoch_train_history\n",
    "    \n",
    "    # val\n",
    "    epoch_val_history = poly_encoder.iteration_epoch(data_loader=val_data_loader, optimizer=optimizer, device=device, train=False, verbose_per_batch=-1)\n",
    "    # compute_scores\n",
    "    val_candidates, val_candidate_embeds = poly_encoder.extract_candidates(device=device, dataset=val_dataset, additional_responses=None, concat_candidates=True, verbose=False)\n",
    "    val_scores = poly_encoder.compute_scores(metrics=metrics, tokenizer=tokenizer, data_loader=val_data_loader, device=device, hits_k=hits_k, name_or_path=name_or_path, candidates=val_candidates, candidate_embeds=val_candidate_embeds)\n",
    "    for metric, metric_score in val_scores.items():\n",
    "        epoch_val_history._add_acc(name=metric, value=metric_score)\n",
    "        \n",
    "    epoch_val_history_str = poly_encoder.verbose_template.format(mode=\"Epoch_val\", device=device, idx=_epoch, num_iters=epoch) + str(epoch_val_history)\n",
    "    print(epoch_val_history_str)\n",
    "    val_history += epoch_val_history\n",
    "\n",
    "    # candidates, candidate_embeds = poly_encoder.extract_candidates(device=device, dataset=train_dataset, additional_responses=None, concat_candidates=True, verbose=True)\n",
    "    candidates, candidate_embeds = train_candidates, train_candidate_embeds\n",
    "    poly_encoder.save(path=model_dir + \"epoch_{}/\".format(_epoch), optimizer=optimizer, tokenizer=tokenizer, candidates=(candidates, candidate_embeds))\n",
    "    with open(model_dir+\"log.txt\", \"a\", encoding=\"utf-8\") as fp: \n",
    "        fp.write(epoch_train_history_str + \"\\n\")\n",
    "        fp.write(epoch_val_history_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract Candidates\n",
    "# epoch = 30\n",
    "# input_model_dir = dataset_dir + \"/model/poly_encoder/v3/concat/{}/\".format(dataset_name)\n",
    "# output_model_dir = dataset_dir + \"/model/poly_encoder/v3/seperate/{}/\".format(dataset_name)\n",
    "# from transformer.models.utils import load_state_dict\n",
    "# for _epoch in range(1, epoch + 1):\n",
    "#     poly_encoder = load_state_dict(object=poly_encoder, path=input_model_dir+\"epoch_{}/\".format(_epoch))\n",
    "#     candidates, candidate_embeds = poly_encoder.extract_candidates(device=device, dataset=train_dataset, additional_responses=None, concat_candidates=False, verbose=True)\n",
    "#     poly_encoder.save(path=output_model_dir+\"epoch_{}/\".format(_epoch), optimizer=optimizer, tokenizer=tokenizer, candidates=(candidates, candidate_embeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PolyEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T07:35:02.891577Z",
     "start_time": "2021-09-27T07:34:48.421293Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing scores: 100%|████████████████████████████████████████████████████████████████| 46/46 [00:14<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "epoch = 50\n",
    "metrics = [\"hits\", \"semantic_score\"]\n",
    "hits_k = [1, 2, 5, 10]\n",
    "name_or_path = \"beomi/kcbert-base\"\n",
    "model_name = \"PolyEncoder\"\n",
    "\n",
    "model_dir = dataset_dir + \"/model/poly_encoder/{dataset_name}/\".format(dataset_name=dataset_name)\n",
    "log_dir = dataset_dir + \"/essay/poly_encoder/{dataset_name}/\".format(dataset_name=dataset_name)\n",
    "init_path(log_dir, reset=True)\n",
    "for _epoch in range(1, epoch+1):\n",
    "    poly_encoder = load_state_dict(object=poly_encoder, path=model_dir+\"epoch_{_epoch}/\".format(_epoch=_epoch))\n",
    "    candidates, candidate_embeds = poly_encoder.extract_candidates(device=device, dataset=test_dataset, additional_responses=None, verbose=True)\n",
    "    scores = poly_encoder.compute_scores(metrics=metrics, tokenizer=tokenizer, data_loader=test_data_loader, device=device, hits_k=hits_k, name_or_path=name_or_path, candidates=candidates, candidate_embeds=candidate_embeds)\n",
    "    output_json = get_score_json(model_name=model_name, dataset_name=dataset_name, test_data_size=len(test_data_loader.dataset), batch_size=batch_size, scores=scores)\n",
    "\n",
    "    # verbose & append log\n",
    "    eval_history = TrainHistory()\n",
    "    loss_dict = dict()\n",
    "    acc_dict = dict()\n",
    "    for metric, metric_score in scores.items():\n",
    "        acc_dict[metric] = metric_score\n",
    "    eval_history.update(loss_dict=loss_dict, acc_dict=acc_dict, lr=-1)\n",
    "    eval_str = poly_encoder.verbose_template.format(mode=\"Eval\", device=device, idx=_epoch, num_iters=epoch) + str(eval_history)\n",
    "    print(eval_str)\n",
    "\n",
    "    with open(log_dir + \"/score_logs.txt\", \"a\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(eval_str + \"\\n\")\n",
    "\n",
    "    # write detailed logs\n",
    "    init_path(log_dir + \"/detailed/\", reset=False)\n",
    "    with open(log_dir + \"/detailed/score_logs_{_epoch}.json\".format(_epoch=_epoch), \"w\", encoding=\"utf-8\") as fp:\n",
    "        json.dumps(output_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"hits\", \"semantic_score\"]\n",
    "hits_k = [1, 2, 5, 10]\n",
    "name_or_path = \"beomi/kcbert-base\"\n",
    "model_name = \"PolyEncoder\"\n",
    "\n",
    "candidates, candidate_embeds = poly_encoder.extract_candidates(device=device, dataset=test_dataset, additional_responses=None, verbose=True)\n",
    "bm25.set_candidates(candidates=candidates)\n",
    "scores = bm25.compute_scores(metrics=metrics, tokenizer=tokenizer, data_loader=test_data_loader, device=device, hits_k=hits_k)\n",
    "\n",
    "eval_history = TrainHistory()\n",
    "loss_dict = dict()\n",
    "acc_dict = dict()\n",
    "for metric, metric_score in scores.items():\n",
    "    acc_dict[metric] = metric_score\n",
    "eval_history.update(loss_dict=loss_dict, acc_dict=acc_dict, lr=-1)\n",
    "eval_str = bm25.verbose_template.format(mode=\"Eval\", device=device, idx=-1, num_iters=-1) + str(eval_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.services.dialog_retriever.poly_encoder import PolyEncoderDialogRetriever\n",
    "service = PolyEncoderDialogRetriever()\n",
    "service.verbose = False\n",
    "_epoch = 10\n",
    "service.set_device(device=device)\n",
    "_model_dir = dataset_dir + \"/model/poly_encoder/{dataset_name}/epoch_{_epoch}/\".format(dataset_name=dataset_name, _epoch=_epoch)\n",
    "service.load_model(model_dir=_model_dir)\n",
    "\n",
    "# print(\"prev candidates size: {} / {}\".format(len(service.candidates), len(service.candidate_embeds)))\n",
    "# candidates, candidate_embeds = service.model.extract_candidates(device=device, dataset=train_dataset, additional_responses=None, concat_candidates=False, verbose=True)\n",
    "# service.set_candidates(candidates=candidates, candidate_embeds=candidate_embeds)\n",
    "print(\"cur candidates size: {} / {}\".format(len(service.candidates), len(service.candidate_embeds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T15:35:44.688213Z",
     "start_time": "2021-09-07T15:35:44.282019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before generate\n",
      "input_ids: 1\n",
      "input_ids: torch.Size([1, 4])\n",
      "output: tensor([[    1,     0, 22465, 22465, 23935, 23935, 23935, 27667, 23935, 23935,\n",
      "         17275, 23935, 23935, 10619,     0,     0, 22465,     0,     0, 23935,\n",
      "             0,     0,     1]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕 안녕하세요하세요하세요와이하세요하세요세요하세요하세요못 안녕하세요'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances = [\n",
    "    \"안녕하세요\",\n",
    "#     \"무슨 일로 저에게 상담을 신청하셨나요?\",\n",
    "#     \"요즘 인간관계가 고민이에요.\",\n",
    "#     \"어떤 고민이죠?\",\n",
    "#     \"친구들이랑 연락도 뜸해지고 자주 못만나서 서먹해지는 것 같아요\",\n",
    "#     \"이래저래 연락하기 힘드신가봐요\",\n",
    "#     \"네, 코로나 때문에 만나질 못해서 더 혼자가 된 느낌이에요.\",\n",
    "#     \"저도 지쳐요.\",\n",
    "#     \"당신도 사람들을 자주 못 만나시나봐요\"\n",
    " ]\n",
    "speaker_ids = [(i+1)%2 for i in range(len(utterances))]\n",
    "min_length = 10\n",
    "top_n = 5\n",
    "\n",
    "outputs = service.infer_next_utterance(utterances=utterances, speaker_ids=speaker_ids,\n",
    "                                       min_length=min_length, top_n=top_n, weight_bm25=False,\n",
    "                                       prev_utterance=None, intersection_tolerance=0.9, max_retry=5)\n",
    "print(\"vanila:\\n\", outputs)\n",
    "\n",
    "outputs = service.infer_next_utterance(utterances=utterances, speaker_ids=speaker_ids,\n",
    "                                       min_length=min_length, top_n=top_n, weight_bm25=True,\n",
    "                                       prev_utterance=None, intersection_tolerance=0.9, max_retry=5)\n",
    "print(\"weighted:\\n\", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.services.dialog_retriever.poly_encoder import PolyEncoderDialogRetriever\n",
    "service = PolyEncoderDialogRetriever()\n",
    "service.verbose = False\n",
    "_epoch = 34\n",
    "service.set_device(device=device)\n",
    "_model_dir = dataset_dir + \"/model/poly_encoder/v2/{dataset_name}/epoch_{_epoch}/\".format(dataset_name=dataset_name, _epoch=_epoch)\n",
    "service.load_model(model_dir=_model_dir)\n",
    "\n",
    "print(\"prev candidates size: {} / {}\".format(len(service.candidates), len(service.candidate_embeds)))\n",
    "candidates, candidate_embeds = service.model.extract_candidates(device=device, dataset=test_dataset, additional_responses=None, concat_candidates=False, verbose=True)\n",
    "service.set_candidates(candidates=candidates, candidate_embeds=candidate_embeds)\n",
    "print(\"cur candidates size: {} / {}\".format(len(service.candidates), len(service.candidate_embeds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.data.utils import simplify_speaker_ids\n",
    "from transformer.utils.common import get_last_index\n",
    "from KoBERTScore import BERTScore\n",
    "from transformer.models.utils import compute_bleu, compute_meteor, compute_rouge, compute_hits, compute_semantic_score\n",
    "\n",
    "def get_metric_inputs(dataset, min_length=1, top_n=10):\n",
    "    for row_idx in range(0, len(dataset.raw_data)):\n",
    "        output = None\n",
    "        \n",
    "        _utterances = dataset.raw_data[row_idx][\"utterances\"]\n",
    "        _speaker_ids = dataset.raw_data[row_idx][\"speaker_ids\"]\n",
    "        _speaker_ids = simplify_speaker_ids(_speaker_ids, user_id=1, model_id=0)\n",
    "        last_index = get_last_index(_speaker_ids, value=1)\n",
    "        utterances = _utterances[:last_index+1]\n",
    "        speaker_ids = _speaker_ids[:last_index+1]\n",
    "        reference = _utterances[last_index+1:]\n",
    "        reference = \" \".join(reference)\n",
    "\n",
    "        try:\n",
    "            context = \" \".join(utterances)\n",
    "            outputs = service.bm25.get_top_n(context=context, n=top_n)\n",
    "            bm25_prediction = [output for output in outputs]\n",
    "            \n",
    "            # unweighted\n",
    "            outputs = service.infer_next_utterance(utterances=utterances, speaker_ids=speaker_ids,\n",
    "                                                   min_length=min_length, top_n=top_n, weight_bm25=False,\n",
    "                                                   prev_utterance=None, intersection_tolerance=0.9, max_retry=5)\n",
    "            unweighted_prediction = [output[0] for output in outputs]\n",
    "\n",
    "            # weighted\n",
    "            outputs = service.infer_next_utterance(utterances=utterances, speaker_ids=speaker_ids,\n",
    "                                                   min_length=min_length, top_n=top_n, weight_bm25=True,\n",
    "                                                   prev_utterance=None, intersection_tolerance=0.9, max_retry=5)\n",
    "            weighted_prediction = [output[0] for output in outputs]\n",
    "\n",
    "            output = {\n",
    "                \"context\": context,\n",
    "                \"reference\": reference,\n",
    "                \"bm25_prediction\": bm25_prediction,\n",
    "                \"unweighted_prediction\": unweighted_prediction,\n",
    "                \"weighted_prediction\": weighted_prediction\n",
    "            }\n",
    "            yield output\n",
    "        except:\n",
    "            yield output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"hits\", \"semantic_score\"]\n",
    "hits_k = [1,2,5,10]\n",
    "name_or_path = \"beomi/kcbert-base\"\n",
    "min_length = 1\n",
    "top_n = max(hits_k)\n",
    "\n",
    "metric_input_gen = get_metric_inputs(dataset=test_dataset, min_length=min_length, top_n=top_n)\n",
    "\n",
    "bm25_predictions = []\n",
    "unweighted_predictions = []\n",
    "weighted_predictions = []\n",
    "references = []\n",
    "for gen_output in tqdm(metric_input_gen):\n",
    "    if gen_output is None: continue\n",
    "    references.append(gen_output[\"reference\"])\n",
    "    bm25_predictions.append(gen_output[\"bm25_prediction\"])\n",
    "    unweighted_predictions.append(gen_output[\"unweighted_prediction\"])\n",
    "    weighted_predictions.append(gen_output[\"weighted_prediction\"])\n",
    "\n",
    "# unweighted_scores\n",
    "bm25_scores = dict()\n",
    "if \"hits\" in metrics:\n",
    "    # hits requires predictions: List[List[any]], references: List[str]\n",
    "    bm25_scores[\"hits\"] = compute_hits(predictions=bm25_predictions, references=references, k=hits_k)\n",
    "if \"semantic_score\" in metrics:\n",
    "    # BERTScore requires predictions: List[str], references: List[str]\n",
    "    _bm25_predictions = [prediction[0] for prediction in bm25_predictions]\n",
    "    poly_encoder.metrics[\"semantic_score\"] = BERTScore(model_name_or_path=name_or_path, best_layer=-1, device=device)\n",
    "    bm25_scores[\"semantic_score\"] = compute_semantic_score(metric=poly_encoder.metrics[\"semantic_score\"], tokenizer=tokenizer, predictions=_bm25_predictions, references=references)\n",
    "print(\"bm25_scores:\", bm25_scores)\n",
    "    \n",
    "# unweighted_scores\n",
    "unweighted_scores = dict()\n",
    "if \"hits\" in metrics:\n",
    "    # hits requires predictions: List[List[any]], references: List[str]\n",
    "    unweighted_scores[\"hits\"] = compute_hits(predictions=unweighted_predictions, references=references, k=hits_k)\n",
    "if \"semantic_score\" in metrics:\n",
    "    # BERTScore requires predictions: List[str], references: List[str]\n",
    "    _unweighted_predictions = [prediction[0] for prediction in unweighted_predictions]\n",
    "    poly_encoder.metrics[\"semantic_score\"] = BERTScore(model_name_or_path=name_or_path, best_layer=-1, device=device)\n",
    "    unweighted_scores[\"semantic_score\"] = compute_semantic_score(metric=poly_encoder.metrics[\"semantic_score\"], tokenizer=tokenizer, predictions=_unweighted_predictions, references=references)\n",
    "print(\"unweighted_scores:\", unweighted_scores)\n",
    "\n",
    "# weighted_scores\n",
    "weighted_scores = dict()\n",
    "if \"hits\" in metrics:\n",
    "    # hits requires predictions: List[List[any]], references: List[str]\n",
    "    weighted_scores[\"hits\"] = compute_hits(predictions=weighted_predictions, references=references, k=hits_k)\n",
    "if \"semantic_score\" in metrics:\n",
    "    # BERTScore requires predictions: List[str], references: List[str]\n",
    "    _weighted_predictions = [prediction[0] for prediction in weighted_predictions]\n",
    "    poly_encoder.metrics[\"semantic_score\"] = BERTScore(model_name_or_path=name_or_path, best_layer=-1, device=device)\n",
    "    weighted_scores[\"semantic_score\"] = compute_semantic_score(metric=poly_encoder.metrics[\"semantic_score\"], tokenizer=tokenizer, predictions=_weighted_predictions, references=references)\n",
    "\n",
    "print(\"weighted_scores:\", weighted_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
